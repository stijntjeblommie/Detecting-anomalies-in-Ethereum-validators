{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63f0401-7640-4cbe-853f-a46376f8b3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 19:53:02.038415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-24 19:53:03.113926: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f257bf8709b94f649ceeb5f63d03ad5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileInput(events=['upload'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e977e05058b4ba2aece0bd891a86cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 1\n",
    "# !pip install scikit-learn==1.2.2 pandas numpy psutil ipywidgets ipyvuetify bqplot seaborn scipy==1.11.4 matplotlib dill joblib imblearn \"ray[tune]>=2.7,<2.10\" scikit-optimize hyperopt tune-sklearn category_encoders optuna tensorflow\n",
    "\n",
    "# Cell 2\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # Print the current working directory\n",
    "# print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# # List all files in the current directory\n",
    "# print(\"Files in the current directory:\", os.listdir())\n",
    "\n",
    "# # Read the CSV file\n",
    "# outlier_models = pd.read_csv(\"outlier_results.csv\")\n",
    "# cluster_models = pd.read_csv(\"cluster_results.csv\")\n",
    "\n",
    "# Cell 3\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Note these are suggestions based on my own research/opinion\n",
    "# Please do your own research\n",
    "attack_details = {\n",
    "    \"Discouragement Attack\": {\n",
    "        \"features\": [\n",
    "            \"Block Number\", \"Timestamp\", \"Slot\", \"Validator Pubkey\", \n",
    "            \"Validator Pool\", \"Validator Name\", \"Gas Earned\", \n",
    "            \"Base Fee per Gas\", \"Gas Limit\", \"Gas Used\", \"N Transactions\", \n",
    "            \"MEVBoost Value\", \"Reverted\"\n",
    "        ],\n",
    "        \"link\": \"https://github.com/ethereum/research/blob/master/papers/discouragement/discouragement.pdf\"\n",
    "    },\n",
    "    \"Time-Bandit Attack\": {\n",
    "        \"features\": [\n",
    "            \"Timestamp\", \"Slot\", \"Validator Pubkey\", \"Proposer Pubkey\", \n",
    "            \"Gas Earned\", \"MEVBoost Value\", \"Reverted\"\n",
    "        ],\n",
    "        \"link\": \"https://arxiv.org/pdf/2110.10086\"\n",
    "    },\n",
    "    \"Single Slot Finality Issues\": {\n",
    "        \"features\": [\n",
    "            \"Block Number\", \"Timestamp\", \"Slot\", \"Validator Pubkey\", \n",
    "            \"Proposer Pubkey\", \"Gas Earned\", \"Reverted\"\n",
    "        ],\n",
    "        \"link\": \"https://notes.ethereum.org/@vbuterin/single_slot_finality#Bad-news-hybrid-consensus-mechanisms-actually-have-many-unavoidable-proble\"\n",
    "    },\n",
    "    \"Hidden-Chain Attacks\": {\n",
    "        \"features\": [\n",
    "            \"Timestamp\", \"Slot\", \"Validator Pubkey\", \"Proposer Pubkey\", \n",
    "            \"Gas Earned\", \"MEVBoost Value\", \"Reverted\"\n",
    "        ],\n",
    "        \"link\": \"https://arxiv.org/pdf/2209.03255\"\n",
    "    },\n",
    "    \"Balancing Attack on Gasper\": {\n",
    "        \"features\": [\n",
    "            \"Timestamp\", \"Slot\", \"Validator Pubkey\", \"Proposer Pubkey\", \n",
    "            \"Gas Earned\", \"Reverted\"\n",
    "        ],\n",
    "        \"link\": \"https://ethresear.ch/t/a-balancing-attack-on-gasper-the-current-candidate-for-eth2s-beacon-chain/8079?u=benjaminion\"\n",
    "    },\n",
    "    \"Avalanche Attack\": {\n",
    "        \"features\": [\n",
    "            \"Block Number\", \"Timestamp\", \"Slot\", \"Validator Pubkey\", \n",
    "            \"Proposer Pubkey\", \"Gas Earned\", \"Reverted\"\n",
    "        ],\n",
    "        \"link\": \"https://arxiv.org/pdf/2203.01315\"\n",
    "    },\n",
    "    \"Reorg Attack\": {\n",
    "        \"features\": [\n",
    "            \"Block Number\", \"Timestamp\", \"Slot\", \"Validator Pubkey\", \n",
    "            \"Proposer Pubkey\", \"Gas Earned\", \"Reverted\"\n",
    "        ],\n",
    "        \"link\": \"https://arxiv.org/pdf/2009.04987\"\n",
    "    },\n",
    "    \"Unrealized Justification Reorgs\": {\n",
    "        \"features\": [\n",
    "            \"Block Number\", \"Timestamp\", \"Slot\", \"Validator Pubkey\", \n",
    "            \"Proposer Pubkey\", \"Gas Earned\", \"Reverted\"\n",
    "        ],\n",
    "        \"link\": \"https://notes.ethereum.org/@adiasg/unrealized-justification\"\n",
    "    },\n",
    "    \"Decoy Flip-Flop Attack\": {\n",
    "        \"features\": [\n",
    "            \"Validator Pubkey\", \"Validator Name\", \"Timestamp\", \n",
    "            \"Slot\", \"Reverted\"\n",
    "        ],\n",
    "        \"link\": \"https://ethresear.ch/t/decoy-flip-flop-attack-on-lmd-ghost/6001\"\n",
    "    },\n",
    "    \"MEV-Boost Relay Incident\": {\n",
    "        \"features\": [\n",
    "            \"Timestamp\", \"Slot\", \"Validator Pubkey\", \"Proposer Pubkey\", \n",
    "            \"Gas Earned\", \"MEVBoost Value\", \"Reverted\"\n",
    "        ],\n",
    "        \"link\": \"https://collective.flashbots.net/t/post-mortem-april-3rd-2023-mev-boost-relay-incident-and-related-timing-issue/1540\"\n",
    "    },\n",
    "    \"Rogue Key Attacks\": {\n",
    "        \"features\": [\n",
    "            \"Validator Pubkey\", \"Timestamp\", \"Slot\", \"Reverted\", \n",
    "            \"Gas Earned\"\n",
    "        ],\n",
    "        \"link\": \"https://hackmd.io/@benjaminion/bls12-381#Rogue-key-attacks\"\n",
    "    },\n",
    "    \"Bouncing Attack on FFG\": {\n",
    "        \"features\": [\n",
    "            \"Validator Pubkey\", \"Proposer Pubkey\", \"Timestamp\", \n",
    "            \"Slot\", \"Reverted\"\n",
    "        ],\n",
    "        \"link\": \"https://ethresear.ch/t/analysis-of-bouncing-attack-on-ffg/6113?u=benjaminion\"\n",
    "    },\n",
    "    \"Balancing Attack LMD Edition\": {\n",
    "        \"features\": [\n",
    "            \"Validator Pubkey\", \"Validator Name\", \"Timestamp\", \n",
    "            \"Slot\", \"Reverted\"\n",
    "        ],\n",
    "        \"link\": \"https://ethresear.ch/t/balancing-attack-lmd-edition/11853?u=benjaminion\"\n",
    "    },\n",
    "    \"Proposer Boost\": {\n",
    "        \"features\": [\n",
    "            \"Timestamp\", \"Slot\", \"Validator Pubkey\", \"Proposer Pubkey\", \n",
    "            \"Gas Earned\", \"MEVBoost Value\", \"Reverted\"\n",
    "        ],\n",
    "        \"link\": \"https://github.com/ethereum/consensus-specs/pull/496#issuecomment-457546253\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cell 4\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "import ray\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.pipeline import Pipeline, clone\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, \n",
    "    OneHotEncoder, \n",
    "    MinMaxScaler, \n",
    "    OrdinalEncoder, \n",
    "    LabelEncoder, \n",
    "    FunctionTransformer, \n",
    "    PowerTransformer, \n",
    "    QuantileTransformer, \n",
    "    RobustScaler, \n",
    "    MaxAbsScaler, \n",
    "    Normalizer\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import (\n",
    "    PCA, \n",
    "    SparsePCA, \n",
    "    TruncatedSVD, \n",
    "    FastICA\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    IsolationForest, \n",
    "    RandomForestClassifier, \n",
    "    AdaBoostClassifier, \n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, OneClassSVM\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    KFold, \n",
    "    LeaveOneOut, \n",
    "    HalvingGridSearchCV, \n",
    "    HalvingRandomSearchCV\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_selection import (\n",
    "    RFE, \n",
    "    SelectKBest, \n",
    "    VarianceThreshold, \n",
    "    f_classif, \n",
    "    f_regression\n",
    ")\n",
    "from custom_transformers import *\n",
    "from category_encoders import TargetEncoder\n",
    "from tune_sklearn import TuneSearchCV, TuneGridSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.base import BaseSampler\n",
    "from scipy import sparse\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "selector = make_column_selector\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),  # Impute missing values with 0\n",
    "    ('scaler', StandardScaler()) # Scale features\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='none')),  # Impute missing values with 'none'\n",
    "    ('onehot', OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)) # One-hot encode categorical features\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, selector(dtype_include=np.number)),\n",
    "    ('cat', categorical_transformer, selector(dtype_include='object'))\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline_cleaning2 = Pipeline(steps=[\n",
    "    ('drop_columns', DropColumns()),\n",
    "    ('remove_duplicates', RemoveDuplicates()),\n",
    "    ('calculate_gas', CalculateEarnedGas()),\n",
    "    ('feature_engineering', FeatureEngineering()),\n",
    "])\n",
    "preprocessing_pipeline_un = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "])\n",
    "\n",
    "\n",
    "# Cell 5\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, widgets, HBox, VBox, HTML\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import dill\n",
    "import os\n",
    "from minio import Minio\n",
    "import tempfile\n",
    "from minio.error import S3Error\n",
    "from sklearn.metrics import make_scorer, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
    "\n",
    "selected_dict = {\n",
    "'mev': ['relay', 'builder', 'mevboost_value'],\n",
    "'amount': ['hash', 'n_transactions', 'transaction_frequency', 'time_span'],\n",
    "'general': ['block_number', 'validator_pool', 'validator_name', 'slot'],\n",
    "'monetary': ['burnt', 'max_fee_per_gas', 'max_priority_fee_per_gas', 'base_fee_per_gas', 'gas_limit', 'gas_used', 'gas_earned'],\n",
    "'compliant': ['ofac_compliant', 'reverted']\n",
    "}\n",
    "\n",
    "feature_groups = list(selected_dict.keys())\n",
    "\n",
    "param_s3_server = \"scruffy.lab.uvalight.net:9000\"\n",
    "param_s3_bucket = \"\"\n",
    "param_s3_user_prefix = \"\"\n",
    "param_s3_access_key = \"\"\n",
    "param_s3_secret_key = \"\"\n",
    "\n",
    "minio_client = Minio(param_s3_server, access_key=param_s3_access_key, secret_key=param_s3_secret_key, secure=True)\n",
    "\n",
    "def list_objects_from_minio(bucket_name, prefix):\n",
    "    objects = []\n",
    "    try:\n",
    "        objects = minio_client.list_objects(bucket_name, prefix=prefix, recursive=True)\n",
    "    except S3Error as e:\n",
    "        print(f\"Error listing objects from MinIO: {e}\")\n",
    "    return objects\n",
    "\n",
    "def parse_filename(filename):\n",
    "    try:\n",
    "        if not filename.endswith(\".pkl\"):\n",
    "            return None, None\n",
    "        model_type, groups_str = filename.split(\"_\", 1)\n",
    "        groups_str = groups_str.replace(\".pkl\", \"\")\n",
    "        return model_type, groups_str\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing filename {filename}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def get_best_models_from_minio(selected):\n",
    "    best_models = {}\n",
    "    final = {}\n",
    "    objects = list_objects_from_minio(param_s3_bucket, f\"{param_s3_user_prefix}/vl-openlab/icos-naavre-demo/\")\n",
    "    selected_set = set(selected)\n",
    "    \n",
    "    for obj in objects:\n",
    "        filename = os.path.basename(obj.object_name)\n",
    "        model_type, groups = parse_filename(filename)\n",
    "     \n",
    "        if groups:\n",
    "            \n",
    "            if groups not in best_models:\n",
    "                best_models[groups] = [model_type]\n",
    "            else:\n",
    "                best_models[groups].append(model_type)\n",
    "    for groups, model_types in best_models.items():\n",
    "        if selected_set == eval(groups):\n",
    "            final[groups] = model_types\n",
    "    return final\n",
    "\n",
    "\n",
    "def interactive_pca_tsne_plot(df, output_pca_tsne, get_selected_features):\n",
    "    def plot_pca_tsne(_):\n",
    "        with output_pca_tsne:\n",
    "            clear_output()\n",
    "            display(widgets.HTML(value=\"<b>Loading plots, please wait...</b>\"))\n",
    "\n",
    "            selected_features, groups = get_selected_features()\n",
    "\n",
    "            if len(groups) < 3:\n",
    "                clear_output(wait=True)\n",
    "                display(widgets.HTML(value=\"<b>Please select at least three feature groups.</b>\"))\n",
    "                return\n",
    "\n",
    "            available_features = [feature for feature in selected_features if feature in df.columns]\n",
    "            missing_features = [feature for feature in selected_features if feature not in df.columns]\n",
    "\n",
    "            if len(available_features) < 2:\n",
    "                clear_output(wait=True)\n",
    "                display(widgets.HTML(value=\"<b>Please select at least two valid features. Missing features: {}</b>\".format(\", \".join(missing_features))))\n",
    "                return\n",
    "\n",
    "            selected_features = df[available_features]\n",
    "            if selected_features.empty or selected_features.isnull().all().all():\n",
    "                clear_output(wait=True)\n",
    "                display(widgets.HTML(value=\"<b>No valid data points available for the selected features. Choose more features.</b>\"))\n",
    "                return\n",
    "\n",
    "            sample_size = min(1000, len(selected_features))\n",
    "            selected_features_sampled = selected_features.sample(n=sample_size)\n",
    "            selected_features_encoded = pd.get_dummies(selected_features_sampled)\n",
    "            scaler = StandardScaler()\n",
    "            standardized_data = scaler.fit_transform(selected_features_encoded)\n",
    "\n",
    "            if standardized_data.shape[0] < 2 or standardized_data.shape[1] < 2:\n",
    "                clear_output()\n",
    "                display(widgets.HTML(value=\"<b>Insufficient data points or features for PCA and t-SNE analysis.</b>\"))\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                pca = PCA(n_components=2)\n",
    "                pca_result = pca.fit_transform(standardized_data)\n",
    "                df_pca = pd.DataFrame(pca_result, columns=['PCA1', 'PCA2'])\n",
    "\n",
    "                pca_loadings = pd.DataFrame(pca.components_.T, columns=['PCA1', 'PCA2'], index=selected_features_encoded.columns)\n",
    "                pca_loadings['Importance'] = pca_loadings.abs().mean(axis=1)\n",
    "                pca_loadings = pca_loadings.sort_values(by='Importance', ascending=False)\n",
    "            except Exception as e:\n",
    "                clear_output(wait=True)\n",
    "                display(widgets.HTML(value=\"<b>Error in PCA or t-SNE computation. Not enough data. Please select more or different features.</b>\"))\n",
    "                return\n",
    "\n",
    "            try:    \n",
    "                perplexity = min(30, len(selected_features_encoded) - 1)\n",
    "                tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "                tsne_result = tsne.fit_transform(standardized_data)\n",
    "                df_tsne = pd.DataFrame(tsne_result, columns=['tSNE1', 'tSNE2'])\n",
    "            except Exception as e:\n",
    "                clear_output(wait=True)\n",
    "                display(widgets.HTML(value=\"<b>Error in PCA or t-SNE computation. Not enough data. Please select more or different features.</b>\"))\n",
    "                return\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(df_pca['PCA1'], df_pca['PCA2'])\n",
    "            plt.title('PCA Plot')\n",
    "            plt.xlabel('PCA1')\n",
    "            plt.ylabel('PCA2')\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.scatter(df_tsne['tSNE1'], df_tsne['tSNE2'])\n",
    "            plt.title('t-SNE Plot')\n",
    "            plt.xlabel('tSNE1')\n",
    "            plt.ylabel('tSNE2')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            display(pca_loadings.head(5))\n",
    "\n",
    "    plot_button = widgets.Button(description=\"Plot PCA and t-SNE\")\n",
    "    plot_button.on_click(plot_pca_tsne)\n",
    "\n",
    "    return VBox([plot_button, output_pca_tsne])\n",
    "\n",
    "def create_tabs(supervised_models, unsupervised_models, df):\n",
    "    group_checkboxes_tab1 = [widgets.Checkbox(value=False, description=group) for group in feature_groups]\n",
    "    group_checkboxes_tab2 = [widgets.Checkbox(value=False, description=group) for group in feature_groups]\n",
    "\n",
    "    group_vbox_tab1 = VBox(group_checkboxes_tab1)\n",
    "    group_vbox_tab2 = VBox(group_checkboxes_tab2)\n",
    "\n",
    "    def get_selected_features_tab1():\n",
    "        selected_groups = [cb.description for cb in group_checkboxes_tab1 if cb.value]\n",
    "        selected_features = list(itertools.chain.from_iterable([selected_dict[group] for group in selected_groups]))\n",
    "        return selected_features, selected_groups\n",
    "\n",
    "    def get_selected_features_tab2():\n",
    "        selected_groups = [cb.description for cb in group_checkboxes_tab2 if cb.value]\n",
    "        selected_features = list(itertools.chain.from_iterable([selected_dict[group] for group in selected_groups]))\n",
    "        return selected_features, selected_groups\n",
    "\n",
    "    output_pca_tsne_tab1 = widgets.Output()\n",
    "    output_pca_tsne_tab2 = widgets.Output()\n",
    "\n",
    "    interactive_plot_widget_tab1 = interactive_pca_tsne_plot(df, output_pca_tsne_tab1, lambda: get_selected_features_tab1())\n",
    "    interactive_plot_widget_tab2 = interactive_pca_tsne_plot(df, output_pca_tsne_tab2, lambda: get_selected_features_tab2())\n",
    "\n",
    "    dropdown_attack = widgets.Dropdown(\n",
    "        options=list(attack_details.keys()),\n",
    "        description='Attacks:'\n",
    "    )\n",
    "\n",
    "    output_attack = widgets.Output()\n",
    "\n",
    "    def update_attack_output(change):\n",
    "        with output_attack:\n",
    "            output_attack.clear_output()\n",
    "            attack = change['new']\n",
    "            details = attack_details[attack]\n",
    "            print(f\"Attack: {attack}\")\n",
    "            print(\"Associated Features:\")\n",
    "            for feature in details[\"features\"]:\n",
    "                print(f\"- {feature}\")\n",
    "            print(f\"\\nLink to Article: {details['link']}\")\n",
    "\n",
    "    dropdown_attack.observe(update_attack_output, names='value')\n",
    "\n",
    "    type_toggle_tab1 = widgets.ToggleButtons(\n",
    "        options=['Supervised', 'Unsupervised'],\n",
    "        description='Type',\n",
    "        disabled=False,\n",
    "        button_style='',\n",
    "    )\n",
    "\n",
    "    type_toggle_tab2 = widgets.ToggleButtons(\n",
    "        options=['Supervised', 'Unsupervised'],\n",
    "        description='Type',\n",
    "        disabled=False,\n",
    "        button_style='',\n",
    "    )\n",
    "    output_supervised = widgets.Output()\n",
    "    def on_toggle_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            if not tabs.selected_index:\n",
    "                with output_tab1:\n",
    "                    clear_output()\n",
    "            else:\n",
    "                with output_tab2:\n",
    "                    clear_output()\n",
    "                with output_supervised:\n",
    "                    clear_output()\n",
    "                    if type_toggle_tab2.value ==  \"Supervised\":\n",
    "                        display(supervised_models)\n",
    "                    else:\n",
    "                        display(unsupervised_models)\n",
    "            \n",
    "\n",
    "    type_toggle_tab1.observe(on_toggle_change)\n",
    "\n",
    "    type_toggle_tab2.observe(on_toggle_change)\n",
    "    \n",
    "    confirm_button_tab1 = widgets.Button(\n",
    "        description='Review Selection',\n",
    "        disabled=False,\n",
    "        button_style='',\n",
    "        tooltip='Click to review selection',\n",
    "        icon='check'\n",
    "    )\n",
    "\n",
    "    confirm_button_tab2 = widgets.Button(\n",
    "        description='Review Selection',\n",
    "        disabled=False,\n",
    "        button_style='',\n",
    "        tooltip='Click to review selection',\n",
    "        icon='check'\n",
    "    )\n",
    "\n",
    "    output_tab1 = widgets.Output()\n",
    "    output_tab2 = widgets.Output()\n",
    "\n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=list(unsupervised_models['Model']),\n",
    "        description='Select Model',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    def on_model_dropdown_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with output_tab2:\n",
    "                clear_output()\n",
    "                analysis_type = type_toggle_tab2.value\n",
    "\n",
    "\n",
    "    model_dropdown.observe(on_model_dropdown_change)\n",
    "    def load_model(groups_set, analysis_type, model=None):\n",
    "        model_type = 'HalvingRandomSearchCV'\n",
    "\n",
    "        if groups_set ==[] or len(groups_set) < 3:\n",
    "            print(\"No model can be found based on your choice, select at least three feature groups\")\n",
    "            return None\n",
    "        if analysis_type == 'Supervised':\n",
    "            result_dict = get_best_models_from_minio(groups_set)\n",
    "            groups = [key for key, value in result_dict.items() if model_type in value][0]\n",
    "        else:\n",
    "            result_dict = get_best_models_from_minio(groups_set)\n",
    "            groups = [key for key, value in result_dict.items() if model_type not in value]\n",
    "\n",
    "            model_type = result_dict[groups[0]][0]\n",
    "            groups = groups[0]\n",
    "        \n",
    "        if model_type:\n",
    "            if model:\n",
    "                model_type = model\n",
    "            feature = f\"{model_type}_{groups}.pkl\"\n",
    "            object_name = f\"{param_s3_user_prefix}/vl-openlab/icos-naavre-demo/{feature}\"\n",
    "\n",
    "            try:\n",
    "                with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "                    minio_client.fget_object(param_s3_bucket, object_name, temp_file.name)\n",
    "                    temp_file_path = temp_file.name\n",
    "                    print(temp_file_path)\n",
    "                with open(temp_file_path, 'rb') as file:\n",
    "                    deserialized_model = dill.load(file)\n",
    "                    \n",
    "                print(\"Model loaded and ready to use.\")\n",
    "                display(deserialized_model)\n",
    "                \n",
    "                return deserialized_model\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading serialized model from MinIO: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"No best model found for the selected groups.\")\n",
    "            return None\n",
    "\n",
    "    def on_button_click_tab1(b):\n",
    "        with output_tab1:\n",
    "            clear_output()\n",
    "            selected_features, selected_groups = get_selected_features_tab1()\n",
    "            analysis_type = type_toggle_tab1.value\n",
    "            deserialized_model = load_model(selected_groups, analysis_type)\n",
    "            if deserialized_model:  \n",
    "                if analysis_type == \"Supervised\":\n",
    "                    X = df.drop(columns=\"ofac_compliant\")\n",
    "                    X['mevboost_value'] = X['mevboost_value'].fillna(0)\n",
    "                    X['relay'] = X['relay'].fillna('none')\n",
    "                    X['builder'] = X['builder'].fillna('none')\n",
    "                    y = df[\"ofac_compliant\"]\n",
    "                    y_pred = deserialized_model.predict(X)\n",
    "\n",
    "                    classification_rep = classification_report(y, y_pred)\n",
    "                    print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "                    accuracy = accuracy_score(y, y_pred)\n",
    "                    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "                    precision = precision_score(y, y_pred)\n",
    "                    print(\"Precision:\", precision)\n",
    "\n",
    "                    recall = recall_score(y, y_pred)\n",
    "                    print(\"Recall:\", recall)\n",
    "\n",
    "                    f1 = f1_score(y, y_pred)\n",
    "                    print(\"F1 Score:\", f1)\n",
    "\n",
    "                    conf_matrix = confusion_matrix(y, y_pred)\n",
    "                    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "                    plt.figure(figsize=(10, 7))\n",
    "                    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "                    plt.title('Confusion Matrix')\n",
    "                    plt.xlabel('Predicted')\n",
    "                    plt.ylabel('True')\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # # X_scaled = preprocessing_pipeline_un.fit_transform(X)\n",
    "\n",
    "                    # X['is_anomaly'] = y_pred\n",
    "\n",
    "                    # tsne = TSNE(n_components=2, random_state=42)\n",
    "                    # X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "                    # plt.figure(figsize=(14, 7))\n",
    "                    # plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c='blue', label='Compliant', alpha=0.5)\n",
    "                    # plt.scatter(X_tsne[X['is_anomaly'] == 1][:, 0], X_tsne[X[['is_anomaly'] == 1][:, 1], c='red', label='OFAC Non-Compliant', alpha=0.5)\n",
    "                    # plt.title('t-SNE plot of MevBoost Value with OFAC Non-Compliant Anomalies')\n",
    "                    # plt.xlabel('t-SNE Component 1')\n",
    "                    # plt.ylabel('t-SNE Component 2')\n",
    "                    # plt.legend()\n",
    "                    # plt.show()\n",
    "                else:\n",
    "                    if hasattr(deserialized_model, 'named_steps'):\n",
    "                        X = df.drop(columns=\"ofac_compliant\")\n",
    "                        X['mevboost_value'] = X['mevboost_value'].fillna(0)\n",
    "                        X['relay'] = X['relay'].fillna('none')\n",
    "                        X['builder'] = X['builder'].fillna('none')\n",
    "                        y = df[\"ofac_compliant\"]\n",
    "                        y_pred = deserialized_model.predict(X)\n",
    "\n",
    "                        classification_rep = classification_report(y, y_pred)\n",
    "                        print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "                        accuracy = accuracy_score(y, y_pred)\n",
    "                        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "                        precision = precision_score(y, y_pred)\n",
    "                        print(\"Precision:\", precision)\n",
    "\n",
    "                        recall = recall_score(y, y_pred)\n",
    "                        print(\"Recall:\", recall)\n",
    "\n",
    "                        f1 = f1_score(y, y_pred)\n",
    "                        print(\"F1 Score:\", f1)\n",
    "\n",
    "                        conf_matrix = confusion_matrix(y, y_pred)\n",
    "                        print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "                        plt.figure(figsize=(10, 7))\n",
    "                        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "                        plt.title('Confusion Matrix')\n",
    "                        plt.xlabel('Predicted')\n",
    "                        plt.ylabel('True')\n",
    "                        plt.show()\n",
    "                    else:\n",
    "                        sample_size = min(1000, len(df))\n",
    "                        df2 = df.sample(sample_size)\n",
    "                        X_new = preprocessing_pipeline_un.fit_transform(df2)\n",
    "\n",
    "                        y_new = df2['ofac_compliant']\n",
    "                        if hasattr(deserialized_model, 'fit_predict'):\n",
    "                            clusters = deserialized_model.fit_predict(X_new)\n",
    "\n",
    "                        else:\n",
    "                            clusters = deserialized_model.predict(X_new)\n",
    "                        numerical_columns = preprocessor.named_transformers_['num'].named_steps['imputer'].feature_names_in_\n",
    "                        categorical_columns = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(preprocessor.named_transformers_['cat'].named_steps['imputer'].feature_names_in_)\n",
    "\n",
    "                        all_columns = list(numerical_columns) + list(categorical_columns) + [\"ofac_compliant\"]\n",
    "\n",
    "                        X_new_with_clusters = pd.DataFrame(X_new, columns=all_columns)\n",
    "                        X_new_with_clusters['Cluster'] = clusters\n",
    "\n",
    "                        pca = PCA(n_components=2)\n",
    "                        X_pca = pca.fit_transform(X_new)\n",
    "\n",
    "                        plt.figure(figsize=(10, 6))\n",
    "                        for cluster in np.unique(clusters):\n",
    "                            cluster_points = X_pca[clusters == cluster]\n",
    "                            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster}')\n",
    "\n",
    "                        unique, counts = np.unique(clusters, return_counts=True)\n",
    "                        anomaly_cluster = unique[np.argmin(counts)]\n",
    "                        anomalies = X_pca[clusters == anomaly_cluster]\n",
    "\n",
    "                        plt.scatter(anomalies[:, 0], anomalies[:, 1], color='red', label='Anomalies', edgecolors='k')\n",
    "                        plt.xlabel('PCA Component 1')\n",
    "                        plt.ylabel('PCA Component 2')\n",
    "                        plt.title('Clusters and Anomalies')\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "\n",
    "                        df_pca = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2'])\n",
    "\n",
    "                        pca_loadings = pd.DataFrame(pca.components_.T, columns=['PCA1', 'PCA2'], index=all_columns)\n",
    "                        pca_loadings['Importance'] = pca_loadings.abs().mean(axis=1)\n",
    "                        pca_loadings = pca_loadings.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "                        display(pca_loadings.head(5))\n",
    "\n",
    "                print(f'Selected Features: {selected_features}')\n",
    "                print(f'Analysis Type: {analysis_type}')\n",
    "            else:\n",
    "                print(\"No features selected\")\n",
    "\n",
    "    def on_button_click_tab2(b):\n",
    "        with output_tab2:\n",
    "            clear_output()\n",
    "            selected_features, selected_groups = get_selected_features_tab2()\n",
    "            selected_model = model_dropdown.value\n",
    "            analysis_type = type_toggle_tab2.value\n",
    "            if analysis_type == \"Unsupervised\":\n",
    "                deserialized_model = load_model(selected_groups, analysis_type,model=selected_model)\n",
    "            else:\n",
    "                deserialized_model = load_model(selected_groups, analysis_type)\n",
    "            if deserialized_model:  \n",
    "                if analysis_type == \"Supervised\":\n",
    "                    X = df.drop(columns=\"ofac_compliant\")\n",
    "                    X['mevboost_value'] = X['mevboost_value'].fillna(0)\n",
    "                    X['relay'] = X['relay'].fillna('none')\n",
    "                    X['builder'] = X['builder'].fillna('none')\n",
    "                    y = df[\"ofac_compliant\"]\n",
    "\n",
    "                    y_pred = deserialized_model.predict(X)\n",
    "\n",
    "                    classification_rep = classification_report(y, y_pred)\n",
    "                    print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "                    accuracy = accuracy_score(y, y_pred)\n",
    "                    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "                    precision = precision_score(y, y_pred)\n",
    "                    print(\"Precision:\", precision)\n",
    "\n",
    "                    recall = recall_score(y, y_pred)\n",
    "                    print(\"Recall:\", recall)\n",
    "\n",
    "                    f1 = f1_score(y, y_pred)\n",
    "                    print(\"F1 Score:\", f1)\n",
    "\n",
    "                    conf_matrix = confusion_matrix(y, y_pred)\n",
    "                    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "                    # X_scaled = preprocessing_pipeline_un.fit_transform(X)\n",
    "\n",
    "                    # X_scaled['is_anomaly'] = y_pred\n",
    "\n",
    "                    # tsne = TSNE(n_components=2, random_state=42)\n",
    "                    # X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "                    # plt.figure(figsize=(14, 7))\n",
    "                    # plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c='blue', label='Compliant', alpha=0.5)\n",
    "                    # plt.scatter(X_tsne[df['is_anomaly'] == 1][:, 0], X_tsne[df['is_anomaly'] == 1][:, 1], c='red', label='OFAC Non-Compliant', alpha=0.5)\n",
    "                    # plt.title('t-SNE plot of MevBoost Value with OFAC Non-Compliant Anomalies')\n",
    "                    # plt.xlabel('t-SNE Component 1')\n",
    "                    # plt.ylabel('t-SNE Component 2')\n",
    "                    # plt.legend()\n",
    "                    # plt.show()\n",
    "                else:\n",
    "                    sample_size = min(1000, len(df))\n",
    "                    df = df.sample(sample_size)\n",
    "                    X_new = preprocessing_pipeline_un.fit_transform(df)\n",
    "                    y_new = df['ofac_compliant']\n",
    "\n",
    "                    if hasattr(deserialized_model, 'fit_predict'):\n",
    "                        clusters = deserialized_model.fit_predict(X_new)\n",
    "                    else:\n",
    "                        clusters = deserialized_model.predict(X_new)\n",
    "                    numerical_columns = preprocessor.named_transformers_['num'].named_steps['imputer'].feature_names_in_\n",
    "                    categorical_columns = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(preprocessor.named_transformers_['cat'].named_steps['imputer'].feature_names_in_)\n",
    "\n",
    "                    all_columns = list(numerical_columns) + list(categorical_columns) + [\"ofac_compliant\"]\n",
    "\n",
    "                    X_new_with_clusters = pd.DataFrame(X_new, columns=all_columns)\n",
    "                    X_new_with_clusters['Cluster'] = clusters\n",
    "\n",
    "                    pca = PCA(n_components=2)\n",
    "                    sample_size = min(1000, len(X_new_with_clusters))\n",
    "                    X_pca = pca.fit_transform(X_new_with_clusters.sample(sample_size))\n",
    "\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    for cluster in np.unique(clusters):\n",
    "                        cluster_points = X_pca[clusters == cluster]\n",
    "                        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster}')\n",
    "\n",
    "                    unique, counts = np.unique(clusters, return_counts=True)\n",
    "                    anomaly_cluster = unique[np.argmin(counts)]\n",
    "                    anomalies = X_pca[clusters == anomaly_cluster]\n",
    "\n",
    "                    plt.scatter(anomalies[:, 0], anomalies[:, 1], color='red', label='Anomalies', edgecolors='k')\n",
    "                    plt.xlabel('PCA Component 1')\n",
    "                    plt.ylabel('PCA Component 2')\n",
    "                    plt.title('Clusters and Anomalies')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "\n",
    "                print(f'Selected Features: {selected_features}')\n",
    "                print(f'Analysis Type: {analysis_type}')\n",
    "                print(f'Selected Model: {selected_model}')\n",
    "            else:\n",
    "                print(\"No features selected\")\n",
    "    confirm_button_tab1.on_click(on_button_click_tab1)\n",
    "    confirm_button_tab2.on_click(on_button_click_tab2)\n",
    "\n",
    "    html_content1 = HTML(value=\"<h3>Section 1: Suggestions for features relevant to an Attack</h3>\")\n",
    "    html_content2 = HTML(value=\"<h3>Section 2: Choose the features for your model</h3>\")\n",
    "    html_content3 = HTML(value=\"<h3>Section 3: Get insight into feature importance with PCA and t-SNE plots</h3>\")\n",
    "    html_content4 = HTML(value=\"<h3>Section 4: Choose between supervised and unsupervised and the best model will be loaded based on your feature choice</h3>\")\n",
    "\n",
    "    html_content5 = HTML(value=\"<h3>Section 5: Use the selected model to detect anomalies</h3>\")\n",
    "    html_content6 = HTML(value=\"<h3>Section 4: Click on supervised or unsupervised to see performance of each model</h3>\")\n",
    "    html_content7 = HTML(value=\"<h3>Section 5: Choose the model you think is most applicable. Currently only limited to Unsupervised models with all of the features selected</h3>\")\n",
    "    html_content8 = HTML(value=\"<h3>Section 6: Use the selected model to detect anomalies</h3>\")\n",
    "    html_content = HTML(value=\"\"\"\n",
    "<div style=\"font-family: Arial, sans-serif; margin: 20px;\">\n",
    "\n",
    "    <p><strong>MEV:</strong> relay, builder, mevboost_value | \n",
    "       <strong>Amount:</strong> hash, n_transactions, transaction_frequency, time_span | \n",
    "       <strong>General:</strong> block_number, validator_pool, validator_name, slot | \n",
    "       <strong>Monetary:</strong> burnt, max_fee_per_gas, max_priority_fee_per_gas, base_fee_per_gas, gas_limit, gas_used, gas_earned | \n",
    "       <strong>Compliant:</strong> ofac_compliant, reverted\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "    tab1 = widgets.VBox([html_content1,dropdown_attack,output_attack, html_content2, html_content, group_vbox_tab1,html_content3, interactive_plot_widget_tab1, html_content4, type_toggle_tab1, html_content5,  confirm_button_tab1, output_tab1])\n",
    "    tab2 = widgets.VBox([html_content1,dropdown_attack,output_attack, html_content2, html_content,group_vbox_tab2,html_content3, interactive_plot_widget_tab2, html_content6, type_toggle_tab2, output_supervised, html_content7, model_dropdown, html_content8, output_tab2, confirm_button_tab2])\n",
    "\n",
    "    tabs = widgets.Tab(children=[tab1, tab2])\n",
    "    tabs.set_title(0, 'Select Parameters')\n",
    "    tabs.set_title(1, 'Select Model by Accuracy')\n",
    "\n",
    "    def on_tab_change(change):\n",
    "        if change['name'] == 'selected_index':\n",
    "            with output_tab1:\n",
    "                clear_output()\n",
    "            with output_tab2:\n",
    "                clear_output()\n",
    "\n",
    "    tabs.observe(on_tab_change, names='selected_index')\n",
    "\n",
    "    comparison_output = widgets.Output()\n",
    "\n",
    "    def compare_results():\n",
    "        with comparison_output:\n",
    "            clear_output()\n",
    "            analysis_type_tab1 = type_toggle_tab1.value\n",
    "            selected_features_tab1, selected_groups_tab1 = get_selected_features_tab1()\n",
    "            \n",
    "            analysis_type_tab2 = type_toggle_tab2.value\n",
    "            selected_features_tab2, selected_groups_tab2 = get_selected_features_tab2()\n",
    "            selected_model_tab2 = model_dropdown.value\n",
    "            \n",
    "            sanctioned = pd.read_csv(\"/home/marvin/informatica/scriptie/sanctioned.csv\")\n",
    "            common_addresses = set(df['block_number']).intersection(set(sanctioned['Block Number']))\n",
    "            common_hash = set(df['hash']).intersection(set(sanctioned['Transaction Hash']))\n",
    "            print(f\"In the dataset there are: {len(common_addresses) + len(common_hash)} fraudulent activities according to the OFAC Sanctions list\")\n",
    "\n",
    "            deserialized_model_tab1 = load_model(selected_groups_tab1, analysis_type_tab1)\n",
    "            X_new = preprocessing_pipeline_un.fit_transform(df)\n",
    "            y_new = df['ofac_compliant']\n",
    "            if deserialized_model_tab1:\n",
    "                if analysis_type_tab1 == \"Unsupervised\":\n",
    "                    if hasattr(deserialized_model_tab1, 'fit_predict'):\n",
    "                        clusters_tab1 = deserialized_model_tab1.fit_predict(X_new)\n",
    "                    else:\n",
    "                        clusters_tab1 = deserialized_model_tab1.predict(X_new)\n",
    "                    anomalies_tab1 = np.where(clusters_tab1 == -1)[0]\n",
    "                    print(anomalies_tab1)\n",
    "                    print(f\"The last selected model in Tab 1 detected {len(anomalies_tab1)} indications/anomalies of illicit behavior\")\n",
    "                else:\n",
    "                    print(\"Tab 1 is set to Supervised. Inspect the confusion matrix.\")\n",
    "                    \n",
    "            else:\n",
    "                print(\"No model selected in Tab1\")\n",
    "\n",
    "            deserialized_model_tab2 = load_model(selected_groups_tab2, analysis_type_tab2, model=selected_model_tab2)\n",
    "            if deserialized_model_tab2:\n",
    "                if analysis_type_tab2 == \"Unsupervised\":\n",
    "                    if hasattr(deserialized_model_tab2, 'fit_predict'):\n",
    "                        clusters_tab2 = deserialized_model_tab2.fit_predict(X_new)\n",
    "                    else:\n",
    "                        clusters_tab2 = deserialized_model_tab2.predict(X_new)\n",
    "                    anomalies_tab2 = np.where(clusters_tab2 == -1)[0]\n",
    "                    print(f\"The last selected model in Tab 2 detected {len(anomalies_tab2)} indications/anomalies of illicit behavior\")\n",
    "                else:\n",
    "                    print(\"Tab 2 is set to Supervised. Inspect the confusion matrix.\")\n",
    "\n",
    "            else:\n",
    "                print(\"No model selected in Tab2\")\n",
    "\n",
    "    compare_button = widgets.Button(\n",
    "        description='Compare Results',\n",
    "        disabled=False,\n",
    "        button_style='',\n",
    "        tooltip='Click to compare results',\n",
    "        icon='check'\n",
    "    )\n",
    "\n",
    "    compare_button.on_click(lambda b: compare_results())\n",
    "\n",
    "    return tabs, compare_button, comparison_output\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "supervised_models = pd.read_csv(\"/home/marvin/informatica/scriptie/all_results (1).csv\")\n",
    "\n",
    "def extract_model(params):\n",
    "    match = re.search(r'model:\\s*(\\w+\\(.*?\\))', params)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "supervised_models['Model'] = supervised_models['Best Params'].apply(extract_model)\n",
    "supervised_models = supervised_models[['Model', 'Best Score']]\n",
    "outlier_models = pd.read_csv(\"/home/marvin/informatica/scriptie/outlier_results (1).csv\")\n",
    "cluster_models = pd.read_csv(\"/home/marvin/informatica/scriptie/cluster_results (1).csv\")\n",
    "unsupervised_models = pd.concat([cluster_models, outlier_models], ignore_index=True)[[\"Model\",\"Best Score\"]]\n",
    "\n",
    "# Cell 6\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from bqplot import *\n",
    "from bqplot.interacts import BrushSelector, FastIntervalSelector\n",
    "from ipywidgets import VBox, Output, HTML\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore, norm, f_oneway\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def create_plotting_widget(blocks):\n",
    "    if 'day' in blocks.columns:\n",
    "        unique_dates = sorted(blocks['day'].unique())\n",
    "    unique_validator_names = sorted(blocks['validator_name'].unique())\n",
    "    \n",
    "    categorical_columns = blocks.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    categorical_columns = [item for item in categorical_columns if item not in ['relay','builder','mevboost_value']]\n",
    "    numeric_columns = blocks.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    unique_values_dict = {col: sorted(blocks[col].dropna().unique()) for col in categorical_columns}\n",
    "    categorical_columns_list = list(unique_values_dict.keys())\n",
    "\n",
    "    output_grid = widgets.Output()\n",
    "    output_info = widgets.Output()\n",
    "    \n",
    "    preset_x = 'block_number'\n",
    "    preset_y = 'gas_earned'\n",
    "    cat = 'validator_name'\n",
    "    x_axis_value = preset_x if preset_x in numeric_columns else numeric_columns[0]\n",
    "    y_axis_value = preset_y if preset_y in numeric_columns else numeric_columns[1]\n",
    "    cat_value = cat if cat in categorical_columns_list else categorical_columns_list[0]\n",
    "\n",
    "    x_axis_dropdown = widgets.Dropdown(\n",
    "        options=numeric_columns,\n",
    "        value=x_axis_value,\n",
    "        description='X-axis:'\n",
    "    )\n",
    "\n",
    "    y_axis_dropdown = widgets.Dropdown(\n",
    "        options=numeric_columns,\n",
    "        value=y_axis_value,\n",
    "        description='Y-axis:'\n",
    "    )\n",
    "\n",
    "    feature_dropdown_x = widgets.Dropdown(\n",
    "        options=numeric_columns,\n",
    "        value=x_axis_value,\n",
    "        description='X of selected interval plot:'\n",
    "    )\n",
    "\n",
    "    feature_dropdown = widgets.Dropdown(\n",
    "        options=numeric_columns,\n",
    "        value=y_axis_value,\n",
    "        description='Y of selected interval plot:'\n",
    "    )\n",
    "\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=categorical_columns_list,\n",
    "        value=cat_value,\n",
    "        description='Analyze:'\n",
    "    )\n",
    "    \n",
    "    current_filtered_blocks = blocks\n",
    "    last_clicked_button = None\n",
    "\n",
    "    def create_figure(filtered_blocks):\n",
    "        x_col = x_axis_dropdown.value \n",
    "        y_col = y_axis_dropdown.value \n",
    "\n",
    "        x_data = filtered_blocks[x_col]\n",
    "        y_data = filtered_blocks[y_col]\n",
    "\n",
    "        x_scale = LinearScale()\n",
    "        y_scale = LinearScale()\n",
    "\n",
    "        scatter = Scatter(\n",
    "            x=x_data,\n",
    "            y=y_data,\n",
    "            scales={'x': x_scale, 'y': y_scale},\n",
    "            colors=[\"blue\"],\n",
    "            selected_style={\"opacity\": \"1\"},\n",
    "            unselected_style={\"opacity\": \"0.2\"}\n",
    "        )\n",
    "\n",
    "        brush_selector = BrushSelector(x_scale=x_scale, y_scale=y_scale, marks=[scatter])\n",
    "\n",
    "        output_plot = Output()\n",
    "\n",
    "        def on_selected(change):\n",
    "            selected_data = []\n",
    "            selected = change['new']\n",
    "            if selected is not None:\n",
    "                min_x, min_y = selected[0]\n",
    "                max_x, max_y = selected[1]\n",
    "                selected_data = filtered_blocks[(x_data >= min_x) & (x_data <= max_x) & (y_data >= min_y) & (y_data <= max_y)]\n",
    "                \n",
    "                feature_col = feature_dropdown.value\n",
    "                feature_x = feature_dropdown_x.value\n",
    "                selected_data = selected_data.sort_values(by=feature_x)\n",
    "\n",
    "                feature_x_data = selected_data[feature_x]\n",
    "                \n",
    "                feature_y_data = selected_data[feature_col]\n",
    "\n",
    "                \n",
    "                window_size = 20\n",
    "                rolling_mean = feature_y_data.rolling(window=window_size, min_periods=1).mean()\n",
    "                rolling_std = feature_y_data.rolling(window=window_size, min_periods=1).std()\n",
    "\n",
    "                outliers = (feature_y_data > rolling_mean + 2 * rolling_std) | (feature_y_data < rolling_mean - 2 * rolling_std)\n",
    "\n",
    "                with output_plot:\n",
    "                \n",
    "                    clear_output()\n",
    "                    \n",
    "                    print(f\"Analysing features: {feature_x}, {feature_col}\")\n",
    "                    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "                    axes[0].plot(feature_x_data.values, feature_y_data.values, 'o', label='Selected Data')\n",
    "\n",
    "                    axes[0].plot(feature_x_data[outliers], feature_y_data[outliers], 'ro', label='Outliers')\n",
    "\n",
    "                    axes[0].set_xlabel(feature_x)\n",
    "                    axes[0].set_ylabel(feature_col)\n",
    "                    axes[0].set_title(f\"Selected Data Plot ({feature_x} vs {feature_col})\")\n",
    "                    axes[0].legend()\n",
    "            \n",
    "                    scatter = axes[1].scatter(feature_x_data.values, feature_y_data.values, c=outliers, cmap='coolwarm', alpha=0.6)\n",
    "                    sns.kdeplot(data=selected_data, x=feature_x, y=feature_col, cmap='Blues', fill=True, bw_adjust=0.5, thresh=0.05, ax=axes[1])\n",
    "                    \n",
    "                    fig.colorbar(scatter, ax=axes[1], label='Anomaly')\n",
    "                \n",
    "                    axes[1].set_title('Data Heatmap with Anomalies Highlighted')\n",
    "                    axes[1].set_xlabel(feature_x)\n",
    "                    axes[1].set_ylabel(feature_col)\n",
    "                \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "\n",
    "        def on_brushing(change):\n",
    "            if not change['new']:\n",
    "                on_selected({'new': brush_selector.selected})\n",
    "                \n",
    "        brush_selector.observe(on_selected, names='selected')\n",
    "\n",
    "        x_ax = Axis(label=x_col, scale=x_scale)\n",
    "        y_ax = Axis(label=y_col, scale=y_scale, orientation='vertical')\n",
    "\n",
    "        fig = Figure(marks=[scatter], axes=[x_ax, y_ax], interaction=brush_selector)\n",
    "\n",
    "        matrix = Output()\n",
    "\n",
    "        with matrix:\n",
    "            num = filtered_blocks.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            corr_matrix = filtered_blocks[num].corr()\n",
    "\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "            plt.title('Correlation Matrix')\n",
    "            plt.show()\n",
    "            print(f\"Analysing features: {x_col}, {y_col}\")\n",
    "        help_label = HTML(\n",
    "                '<div style=\"color: black; font-size: 16px; margin:20px 0px 0px 50px\">\\\n",
    "                  Choose features to analyse the selected interval with time series analysis</div>'\n",
    "        )\n",
    "        help_label2 = HTML(\n",
    "        '<div style=\"color: black; font-size: 16px; margin:20px 0px 0px 50px\">\\\n",
    "          Select an interval with your mouse to generate a detailed plot with statistical anomaly detection/div>'\n",
    "        )\n",
    "\n",
    "        tabs, compare_button, comparison_output = create_tabs(supervised_models, unsupervised_models, blocks)\n",
    "        \n",
    "        display(VBox([matrix, x_axis_dropdown, y_axis_dropdown,help_label2, fig, help_label, feature_dropdown_x, feature_dropdown, output_plot, tabs, compare_button, comparison_output]))\n",
    "    \n",
    "    \n",
    "    def on_button_click(b):\n",
    "        nonlocal current_filtered_blocks, last_clicked_button\n",
    "        with output_info:\n",
    "            clear_output()\n",
    "            last_clicked_button = b.description\n",
    "            print(f\"Clicked the button: {b.description}, {dropdown.value}\")\n",
    "            if dropdown.value == 'day':\n",
    "                selected_data = blocks[blocks['day'] == pd.to_datetime(b.description).date()]\n",
    "                print(selected_data.shape)\n",
    "                if not selected_data.empty:\n",
    "                    current_filtered_blocks = selected_data.sample(min(500, len(selected_data)))\n",
    "                    create_figure(current_filtered_blocks)\n",
    "            else:\n",
    "                feature = dropdown.value\n",
    "                selected_value = b.description\n",
    "                filtered_blocks = blocks[blocks[feature] == selected_value]\n",
    "                print(filtered_blocks.shape)\n",
    "                if not filtered_blocks.empty:\n",
    "                    current_filtered_blocks = filtered_blocks.sample(min(500, len(filtered_blocks)))\n",
    "                    create_figure(current_filtered_blocks)\n",
    "\n",
    "    def create_button_grid(feature):\n",
    "        with output_grid:\n",
    "            clear_output()\n",
    "            unique_values = unique_values_dict[feature]\n",
    "            n_columns = 7\n",
    "            n_rows = (len(unique_values) + n_columns - 1) // n_columns\n",
    "            grid = widgets.GridspecLayout(n_rows=n_rows, n_columns=n_columns, width='100%', grid_gap='10px 5px')\n",
    "\n",
    "            for idx, value in enumerate(unique_values):\n",
    "                row = idx // n_columns\n",
    "                col = idx % n_columns\n",
    "                button = widgets.Button(description=str(value))\n",
    "                button.on_click(on_button_click)\n",
    "                grid[row, col] = button\n",
    "            display(grid)\n",
    "\n",
    "    def on_dropdown_change(change):\n",
    "        create_button_grid(change.new)\n",
    "    \n",
    "    def on_axis_dropdown_change(change):\n",
    "        with output_info:\n",
    "            clear_output()\n",
    "            if last_clicked_button:\n",
    "                print(f\"Clicked the button: {last_clicked_button}\")\n",
    "            create_figure(current_filtered_blocks)\n",
    "\n",
    "    dropdown.observe(on_dropdown_change, names='value')\n",
    "    x_axis_dropdown.observe(on_axis_dropdown_change, names='value')\n",
    "    y_axis_dropdown.observe(on_axis_dropdown_change, names='value')\n",
    "   \n",
    "    display(VBox([dropdown, output_grid,output_info]))\n",
    "    create_button_grid(dropdown.value)\n",
    "\n",
    "# Cell 7\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import time\n",
    "import psutil\n",
    "import ipywidgets as widgets\n",
    "from ipyvuetify.extra import FileInput\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "fi = FileInput(multiple=True) \n",
    "out = widgets.Output()\n",
    "\n",
    "dfs_to_merge = []\n",
    "dfs_to_concat = []\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('drop_columns', DropColumns()),\n",
    "    ('remove_duplicates', RemoveDuplicates()),\n",
    "    ('date_conversion', DateConversion()),\n",
    "    ('calculate_gas', CalculateEarnedGas()),\n",
    "    ('feature_engineering', FeatureEngineering()),\n",
    "])\n",
    "\n",
    "def preprocess_data(file_data):\n",
    "    global dfs_to_merge, dfs_to_concat\n",
    "\n",
    "    dfs_to_merge = []\n",
    "    dfs_to_concat = []\n",
    "\n",
    "    for data, name in file_data:\n",
    "        print(f\"Processing file: {name}\")\n",
    "        if 'blocks' in name and name.endswith('.csv'):\n",
    "            dfs_to_merge.append(pd.read_csv(BytesIO(data)))\n",
    "        \n",
    "        elif name.endswith('.gzip'):\n",
    "            dfs_to_merge.append(pd.read_parquet(BytesIO(data)))\n",
    "        \n",
    "        elif name.endswith('.csv'):\n",
    "            dfs_to_concat.append(pd.read_csv(BytesIO(data)))\n",
    "        \n",
    "        else:\n",
    "            print(f\"Unsupported file type: {name}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Dataframes to merge: {len(dfs_to_merge)}, Dataframes to concatenate: {len(dfs_to_concat)}\")\n",
    "    print(f\"Data is preprocessing...\")\n",
    "    display(pipeline_cleaning2)\n",
    "    display(preprocessor)\n",
    "\n",
    "    if len(dfs_to_merge) > 1:\n",
    "        merged_df = dfs_to_merge[0]\n",
    "        for df in dfs_to_merge[1:]:\n",
    "            merged_df = merged_df.merge(df, on='block_number', how='inner')\n",
    "        \n",
    "        if dfs_to_concat:\n",
    "            concatenated_df = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "            final_df = merged_df.merge(concatenated_df, on='block_number', how='inner')\n",
    "        else:\n",
    "            final_df = merged_df\n",
    "            \n",
    "        \n",
    "        if not final_df.empty:\n",
    "            final_df = preprocessing_pipeline.fit_transform(final_df)\n",
    "            final_df.to_csv('new2.csv', index=False)\n",
    "\n",
    "            print(final_df.shape)\n",
    "          \n",
    "        else:\n",
    "            print(\"Please update blocks.csv or parquet.csv\")\n",
    "            print(\"Merged dataframe is empty. No common 'block_number' to merge on.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"There are no files to merge\")\n",
    "\n",
    "        if dfs_to_concat:\n",
    "            final_df = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "        else:\n",
    "            print(\"Dataframe is empty\")\n",
    "            return\n",
    "    create_plotting_widget(final_df)\n",
    "    \n",
    "def handle_file_upload(change):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        start_total = time.time()\n",
    "        cpu_start_total = psutil.cpu_percent(interval=None)\n",
    "        mem_start_total = psutil.virtual_memory().used / (1024 ** 2)\n",
    "        \n",
    "        files = fi.get_files()\n",
    "        file_data = [(file['file_obj'].read(), file['name']) for file in files]\n",
    "        preprocess_data(file_data)\n",
    "        \n",
    "        end_total = time.time()\n",
    "        cpu_end_total = psutil.cpu_percent(interval=None)\n",
    "        mem_end_total = psutil.virtual_memory().used / (1024 ** 2)\n",
    "        time_taken = end_total - start_total\n",
    "        cpu_usage = cpu_end_total - cpu_start_total\n",
    "        mem_usage = mem_end_total - mem_start_total\n",
    "        \n",
    "        print(f\"Total - Time: {time_taken:.2f} seconds, CPU Usage: {cpu_usage:.2f}%, Memory Usage: {mem_usage:.2f} MB\")\n",
    "\n",
    "fi.observe(handle_file_upload, names='file_info')\n",
    "display(fi, out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
