{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aatth8miLrql",
        "outputId": "6c45658f-b601-4bd3-e126-3a757b97de35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray[tune]<2.10,>=2.7 in /usr/local/lib/python3.10/dist-packages (2.9.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (3.15.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (2.0.3)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (14.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]<2.10,>=2.7) (2023.6.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow>=6.0.1->ray[tune]<2.10,>=2.7) (1.25.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]<2.10,>=2.7) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]<2.10,>=2.7) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]<2.10,>=2.7) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]<2.10,>=2.7) (0.18.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]<2.10,>=2.7) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]<2.10,>=2.7) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]<2.10,>=2.7) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]<2.10,>=2.7) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]<2.10,>=2.7) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]<2.10,>=2.7) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]<2.10,>=2.7) (2024.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ray[tune]<2.10,>=2.7) (1.16.0)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (0.10.2)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: tune-sklearn in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.4.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt) (3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt) (0.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hyperopt) (4.66.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt) (0.10.9.7)\n",
            "Requirement already satisfied: ray[tune]>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from tune-sklearn) (2.9.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.7.1->tune-sklearn) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.7.1->tune-sklearn) (3.15.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.7.1->tune-sklearn) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.7.1->tune-sklearn) (1.0.8)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.7.1->tune-sklearn) (3.20.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.7.1->tune-sklearn) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.7.1->tune-sklearn) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.7.1->tune-sklearn) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.7.1->tune-sklearn) (2.0.3)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.7.1->tune-sklearn) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.7.1->tune-sklearn) (14.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]>=2.7.1->tune-sklearn) (2023.6.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]>=2.7.1->tune-sklearn) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]>=2.7.1->tune-sklearn) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]>=2.7.1->tune-sklearn) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]>=2.7.1->tune-sklearn) (0.18.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]>=2.7.1->tune-sklearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]>=2.7.1->tune-sklearn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]>=2.7.1->tune-sklearn) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]>=2.7.1->tune-sklearn) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]>=2.7.1->tune-sklearn) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]>=2.7.1->tune-sklearn) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]>=2.7.1->tune-sklearn) (2024.6.2)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (0.10.2)\n",
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.10/dist-packages (2.6.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.4.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.11.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: hpbandster in /usr/local/lib/python3.10/dist-packages (0.7.4)\n",
            "Requirement already satisfied: ConfigSpace in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: Pyro4 in /usr/local/lib/python3.10/dist-packages (from hpbandster) (4.82)\n",
            "Requirement already satisfied: serpent in /usr/local/lib/python3.10/dist-packages (from hpbandster) (1.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hpbandster) (1.25.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from hpbandster) (0.14.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from hpbandster) (1.11.4)\n",
            "Requirement already satisfied: netifaces in /usr/local/lib/python3.10/dist-packages (from hpbandster) (0.11.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from ConfigSpace) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ConfigSpace) (4.12.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from ConfigSpace) (10.1.0)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels->hpbandster) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->hpbandster) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels->hpbandster) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels->hpbandster) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels->hpbandster) (1.16.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: minio in /usr/local/lib/python3.10/dist-packages (7.2.7)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (0.3.8)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from minio) (2024.6.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from minio) (2.0.7)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from minio) (23.1.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from minio) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from minio) (4.12.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->minio) (21.2.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->minio) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"ray[tune]>=2.7,<2.10\"\n",
        "!pip install scikit-optimize hyperopt tune-sklearn\n",
        "!pip install scikit-optimize category_encoders joblib matplotlib scikit-learn\n",
        "!pip install hpbandster ConfigSpace\n",
        "!pip install optuna\n",
        "!pip install tensorflow\n",
        "!pip install minio dill"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: the file is named custom_transformers.py and this is the file path '/content/drive/MyDrive/datasets/custom_transformers.py'\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/datasets')\n",
        "from custom_transformers import *\n"
      ],
      "metadata": {
        "id": "oRuBT0JyEVJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store trained model in minIO\n",
        "# (Do not containerize, NaaVRE workflow parameters cell)\n",
        "\n",
        "param_s3_server = \"scruffy.lab.uvalight.net:9000\"\n",
        "param_s3_bucket = \"naa-vre-user-data\"\n",
        "\n",
        "param_s3_user_prefix = \"someren06@gmail.com\"\n",
        "param_s3_access_key = \"lBTiSYIzxxV6gp39pyJl\"\n",
        "param_s3_secret_key = \"6vS4efl1MuEigSkaXOLf0CEuPgxx8k522fw7uopv\"\n",
        "import dill\n",
        "\n",
        "# serialized_model = dill.dumps(best_estimator)"
      ],
      "metadata": {
        "id": "WIVfTJYFErBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload files\n",
        "import os\n",
        "from minio import Minio\n",
        "import slugify\n",
        "\n",
        "from minio import Minio\n",
        "from minio.error import S3Error\n",
        "import io\n",
        "import tempfile\n"
      ],
      "metadata": {
        "id": "n2lB6weREWWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YemvoJFGNGOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a08994-28da-4b87-973e-c7f580999e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: scikit-learn\n",
            "Version: 1.2.2\n",
            "Summary: A set of python modules for machine learning and data mining\n",
            "Home-page: http://scikit-learn.org\n",
            "Author: \n",
            "Author-email: \n",
            "License: new BSD\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: joblib, numpy, scipy, threadpoolctl\n",
            "Required-by: bigframes, category-encoders, fastai, imbalanced-learn, librosa, mlxtend, qudida, scikit-optimize, sklearn-pandas, tune-sklearn, yellowbrick\n"
          ]
        }
      ],
      "source": [
        "!pip show scikit-learn\n",
        "from tune_sklearn import TuneSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ray.tune.schedulers import MedianStoppingRule\n",
        "import numpy as np\n",
        "\n",
        "# Other imports\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3uag2iGW2b4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3NTDwf7iQ5J",
        "outputId": "a51cb970-ed93-43e6-f4ba-13cd152f58a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-d99c1bbd4d3d>:4: DtypeWarning: Columns (18,19,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/drive/MyDrive/datasets/test.csv')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/datasets/test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlYjavTproR1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zq9NLNDvfhvp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import make_column_selector\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from tune_sklearn import TuneSearchCV\n",
        "from imblearn.base import BaseSampler\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# from custom_transformers import DropColumns,\n",
        "\n",
        "\n",
        "selector = make_column_selector\n",
        "\n",
        "# class DropColumns(BaseEstimator, TransformerMixin):\n",
        "#     def __init__(self):\n",
        "#         self.columns_to_drop = []\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         self.columns_to_drop = [col for col in X.columns if 'pubkey' in col or col == 'validator']\n",
        "#         self.columns_to_drop.append('date')\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         print(\"DropColumns transform shape:\", X.shape)\n",
        "#         return X.drop(columns=self.columns_to_drop)\n",
        "\n",
        "# class RemoveDuplicates(BaseEstimator, TransformerMixin):\n",
        "#     def fit(self, X, y=None):\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         print(\"RemoveDuplicates transform shape:\", X.shape)\n",
        "#         return X.drop_duplicates()\n",
        "\n",
        "# class DateConversion(BaseEstimator, TransformerMixin):\n",
        "#     def fit(self, X, y=None):\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         if 'date' in X.columns:\n",
        "#             X['date'] = pd.to_datetime(X['date'])\n",
        "#             X['day'] = X['date'].dt.date\n",
        "#         print(\"DateConversion transform shape:\", X.shape)\n",
        "#         return X\n",
        "\n",
        "# class CalculateEarnedGas(BaseEstimator, TransformerMixin):\n",
        "#     def fit(self, X, y=None):\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         if 'gas_price' in X.columns:\n",
        "#             condition_1 = X['gas_price'] <= 0\n",
        "#             condition_2 = X['max_fee_per_gas'] >= X['base_fee_per_gas'] + X['max_priority_fee_per_gas']\n",
        "\n",
        "#             earned_gas_1 = X['gas'] * (X['base_fee_per_gas'] + X['max_priority_fee_per_gas']) - X['base_fee_per_gas'] * X['gas']\n",
        "#             earned_gas_2 = X['max_fee_per_gas'] * X['gas'] - X['base_fee_per_gas'] * X['gas']\n",
        "#             earned_gas_3 = X['gas'] * X['gas_price'] - X['base_fee_per_gas'] * X['gas']\n",
        "\n",
        "#             X['gas_earned'] = np.where(condition_1 & condition_2, earned_gas_1,\n",
        "#                               np.where(condition_1 & ~condition_2, earned_gas_2, earned_gas_3))\n",
        "\n",
        "#         print(\"CalculateEarnedGas transform shape:\", X.shape)\n",
        "#         return X\n",
        "\n",
        "# class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
        "#     def fit(self, X, y=None):\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         X['burnt'] = X['gas_used'] * X['base_fee_per_gas']\n",
        "#         X['hash'] = X.groupby('block_number')['hash'].transform('nunique')\n",
        "#         X['first_seen_ts'] = X.groupby('block_number')['first_seen_ts'].transform(lambda x: x.max() - x.min())\n",
        "\n",
        "#         X.rename(columns={'first_seen_ts': 'time_span'}, inplace=True)\n",
        "#         X['transaction_frequency'] = np.where(X['time_span'] == 0, 0, X['n_transactions'] / X['time_span'])\n",
        "#         X['reverted'] = X['n_transactions'] - X['hash']\n",
        "#         X.drop(columns=['timestamp', 'gas', 'gas_price'], inplace=True)\n",
        "\n",
        "#         print(\"FeatureEngineering transform shape:\", X.shape)\n",
        "#         return X\n",
        "\n",
        "\n",
        "\n",
        "# class ConditionalSampler(BaseEstimator, TransformerMixin):\n",
        "#     def __init__(self, sampling_method=None):\n",
        "#         self.sampling_method = sampling_method\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X, y=None):\n",
        "#         if self.sampling_method is not None and y is not None and len(set(y)) > 1:\n",
        "#             X_res, y_res = self.sampling_method.fit_resample(X, y)\n",
        "#             return X_res, y_res\n",
        "#         return X, y\n",
        "\n",
        "#     def fit_resample(self, X, y=None):\n",
        "#         return self.transform(X, y)\n",
        "\n",
        "\n",
        "# class ConditionalVarianceThreshold(BaseEstimator, TransformerMixin):\n",
        "#     def __init__(self, threshold=0.0):\n",
        "#         self.threshold = threshold\n",
        "#         self.selector = VarianceThreshold(threshold=self.threshold)\n",
        "#         self.no_features_selected = False\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         try:\n",
        "#             self.selector.fit(X)\n",
        "#             self.variances_ = self.selector.variances_\n",
        "#             self.no_features_selected = not np.any(self.variances_ > self.threshold)\n",
        "#         except ValueError:\n",
        "#             self.no_features_selected = True\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         if self.no_features_selected:\n",
        "#             # If no features meet the threshold, return the original data\n",
        "#             return X\n",
        "#         else:\n",
        "#             return self.selector.transform(X)\n",
        "\n",
        "#     def fit_transform(self, X, y=None, **fit_params):\n",
        "#         return self.fit(X, y).transform(X)\n",
        "\n",
        "#     def get_support(self, indices=False):\n",
        "#         if self.no_features_selected:\n",
        "#             return np.ones(X.shape[1], dtype=bool) if not indices else np.arange(X.shape[1])\n",
        "#         else:\n",
        "#             return self.selector.get_support(indices=indices)\n",
        "\n",
        "# Define columns to exclude for numerical transformation\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),  # Impute missing values with 0\n",
        "    ('scaler', StandardScaler()) # Scale features\n",
        "    #  ('pca', PCA(n_components=0.95))\n",
        "    # ('select', SelectKBest(f_regression, k=10)) # Feature Selection\n",
        "])\n",
        "\n",
        "# categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='none')),  # Impute missing values with 'none'\n",
        "  ('onehot', OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False)) # One-hot encode categorical features\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "# Combine transformers using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numerical_transformer, selector(dtype_include=np.number)),\n",
        "    ('cat', categorical_transformer, selector(dtype_include='object'))\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline_cleaning = Pipeline(steps=[\n",
        "    ('drop_columns', DropColumns()),\n",
        "    ('remove_duplicates', RemoveDuplicates()),\n",
        "    # ('date_conversion', DateConversion()),\n",
        "    ('calculate_gas', CalculateEarnedGas()),\n",
        "    ('feature_engineering', FeatureEngineering()),\n",
        "])\n",
        "preprocessing_pipeline= Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor)\n",
        "])\n",
        "\n",
        "# pipeline_cleaning = Pipeline(steps=[\n",
        "#     ('drop_columns', DropColumns()),\n",
        "#     ('remove_duplicates', RemoveDuplicates()),\n",
        "#     # ('date_conversion', DateConversion()),\n",
        "#     ('calculate_gas', CalculateEarnedGas()),\n",
        "#     ('feature_engineering', FeatureEngineering())\n",
        "# ])\n",
        "# print(\"Before Imputation:\")\n",
        "# display(df.head(1))\n",
        "\n",
        "# # Fit the pipeline to the data and transform it\n",
        "# df_eng = pipeline.fit_transform(df.head(10000))\n",
        "\n",
        "# # Get the columns from the preprocessing steps\n",
        "# num_features = selector(dtype_include=np.number)(df_eng)\n",
        "# cat_features = selector(dtype_include='object')(df_eng)\n",
        "\n",
        "# df_imputed = preprocessing_pipeline.fit_transform(df_eng)\n",
        "\n",
        "# # # Get the columns from the one-hot encoder\n",
        "# ohe = preprocessing_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']\n",
        "# cat_columns = ohe.get_feature_names_out(cat_features)\n",
        "\n",
        "\n",
        "# remaining_columns = [col for col in df_eng.columns if col not in num_features + list(cat_features)]\n",
        "# all_columns = np.concatenate([num_features, cat_columns, remaining_columns])\n",
        "\n",
        "# # # Convert the result back to a DataFrame\n",
        "# df_imputed = pd.DataFrame(df_imputed, columns=all_columns)\n",
        "\n",
        "# print(\"\\nAfter Imputation:\")\n",
        "# display(df_imputed.head())\n",
        "\n",
        "# # Check for missing values\n",
        "# missing_values = df_imputed.isnull().sum()\n",
        "# print(missing_values)\n",
        "# print(df.columns.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU_4Y66j7Tgf",
        "outputId": "87852144-6443-49c5-a67c-6ae1190ca238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hash                              0\n",
            "first_seen_ts                     0\n",
            "gas                               0\n",
            "gas_price                         0\n",
            "max_fee_per_gas                   0\n",
            "max_priority_fee_per_gas          0\n",
            "block_number                      0\n",
            "ofac_compliant                    0\n",
            "timestamp                         0\n",
            "base_fee_per_gas                  0\n",
            "gas_limit                         0\n",
            "gas_used                          0\n",
            "validator_pubkey                  0\n",
            "validator_pool                    0\n",
            "validator_name                    0\n",
            "n_transactions                    0\n",
            "date                              0\n",
            "slot                              0\n",
            "relay                       1708263\n",
            "builder_pubkey              1708263\n",
            "proposer_pubkey                   0\n",
            "mevboost_value              1708263\n",
            "builder                     1708263\n",
            "validator                         0\n",
            "dtype: int64\n",
            "['hash' 'first_seen_ts' 'gas' 'gas_price' 'max_fee_per_gas'\n",
            " 'max_priority_fee_per_gas' 'block_number' 'ofac_compliant' 'timestamp'\n",
            " 'base_fee_per_gas' 'gas_limit' 'gas_used' 'validator_pubkey'\n",
            " 'validator_pool' 'validator_name' 'n_transactions' 'date' 'slot' 'relay'\n",
            " 'builder_pubkey' 'proposer_pubkey' 'mevboost_value' 'builder' 'validator']\n",
            "DropColumns transform shape: (2175353, 24)\n",
            "RemoveDuplicates transform shape: (2175353, 19)\n",
            "CalculateEarnedGas transform shape: (2175353, 20)\n",
            "FeatureEngineering transform shape: (2175353, 20)\n",
            "hash                        0\n",
            "time_span                   0\n",
            "max_fee_per_gas             0\n",
            "max_priority_fee_per_gas    0\n",
            "block_number                0\n",
            "ofac_compliant              0\n",
            "base_fee_per_gas            0\n",
            "gas_limit                   0\n",
            "gas_used                    0\n",
            "validator_pool              0\n",
            "validator_name              0\n",
            "n_transactions              0\n",
            "slot                        0\n",
            "relay                       0\n",
            "mevboost_value              0\n",
            "builder                     0\n",
            "gas_earned                  0\n",
            "burnt                       0\n",
            "transaction_frequency       0\n",
            "reverted                    0\n",
            "dtype: int64\n",
            "['hash' 'time_span' 'max_fee_per_gas' 'max_priority_fee_per_gas'\n",
            " 'block_number' 'ofac_compliant' 'base_fee_per_gas' 'gas_limit' 'gas_used'\n",
            " 'validator_pool' 'validator_name' 'n_transactions' 'slot' 'relay'\n",
            " 'mevboost_value' 'builder' 'gas_earned' 'burnt' 'transaction_frequency'\n",
            " 'reverted']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# df['mevboost_value'] = pd.to_numeric(df['mevboost_value'], errors='coerce')\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "print(df.columns.values)\n",
        "df2 = pipeline_cleaning.fit_transform(df)\n",
        "df2.columns\n",
        "\n",
        "df2['mevboost_value'] = df2['mevboost_value'].fillna(0)\n",
        "df2['relay'] = df2['relay'].fillna('none')\n",
        "df2['builder'] = df2['builder'].fillna('none')\n",
        "\n",
        "missing_values = df2.isnull().sum()\n",
        "print(missing_values)\n",
        "print(df2.columns.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFpATnx8_6ZA"
      },
      "outputs": [],
      "source": [
        "num_columns = df2.select_dtypes(include=np.number).columns\n",
        "cat_columns = df2.select_dtypes(include=['object']).columns\n",
        "print(num_columns)\n",
        "print(cat_columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UATJSICk76bi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNwG5WGn3wz3",
        "outputId": "edd3c145-589e-4b17-9c76-f97e04e7b855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'general', 'amount', 'mev'}, {'amount', 'monetary', 'mev'}, {'amount', 'mev', 'compliant'}, {'general', 'monetary', 'mev'}, {'general', 'mev', 'compliant'}, {'monetary', 'mev', 'compliant'}, {'general', 'amount', 'monetary'}, {'general', 'amount', 'compliant'}, {'amount', 'monetary', 'compliant'}, {'general', 'monetary', 'compliant'}, {'general', 'amount', 'monetary', 'mev'}, {'general', 'amount', 'mev', 'compliant'}, {'amount', 'monetary', 'mev', 'compliant'}, {'general', 'monetary', 'mev', 'compliant'}, {'general', 'amount', 'monetary', 'compliant'}, {'amount', 'monetary', 'compliant', 'mev', 'general'}]\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "# from sklearn.model_selection import KFold, LeaveOneOut, RandomizedSearchCV, HalvingRandomSearchCV\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.decomposition import SparsePCA\n",
        "from tune_sklearn import TuneSearchCV\n",
        "# Generate all possible combinations of the feature groups\n",
        "\n",
        "\n",
        "selected_dict = {\n",
        "    'mev':['relay','builder', 'mevboost_value'],\n",
        "\n",
        "    'amount': ['hash','n_transactions','transaction_frequency', 'time_span'],\n",
        "    'general': ['block_number','validator_pool','validator_name','slot'],\n",
        "    'monetary': ['burnt','max_fee_per_gas','max_priority_fee_per_gas','base_fee_per_gas','gas_limit','gas_used','gas_earned'],\n",
        "\n",
        "    'compliant': ['ofac_compliant', 'reverted']\n",
        "\n",
        "}\n",
        "\n",
        "feature_groups = list(selected_dict.keys())\n",
        "combinations = []\n",
        "for r in range(len(feature_groups) + 1):\n",
        "    combinations.extend(itertools.combinations(feature_groups, r))\n",
        "\n",
        "combinations = [set(comb) for comb in combinations if len(set(comb))>2]\n",
        "print(combinations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjZNKNMdK3ja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cebfa9da-42b7-4aea-a092-08635b72f48e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[[-1.51609303 -0.14525128  0.1803119  ...  0.          1.\n",
            "   1.        ]\n",
            " [-1.51609303 -0.14525128 -0.12593054 ...  0.          1.\n",
            "   1.        ]\n",
            " [-1.51609303 -0.14525128  0.1803119  ...  0.          1.\n",
            "   1.        ]\n",
            " ...\n",
            " [-1.04905777 -0.14525128 -0.11035189 ...  0.          0.\n",
            "   1.        ]\n",
            " [-1.04905777 -0.14525128 -0.12593054 ...  0.          0.\n",
            "   1.        ]\n",
            " [-1.04905777 -0.14525128 -0.12593054 ...  0.          0.\n",
            "   1.        ]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        " X = preprocessing_pipeline.fit_transform(df2)\n",
        " print(type(X))\n",
        " print(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pjtmz4Kziu1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dx26d9aQhDqZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okO2V8wbIQsV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "a025e6ad-3d39-47fd-eb2d-6217c41f56ac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HalvingRandomSearchCV(estimator=Pipeline(steps=[('preprocessor',\n",
              "                                                 ColumnTransformer(remainder='passthrough',\n",
              "                                                                   transformers=[('num',\n",
              "                                                                                  Pipeline(steps=[('imputer',\n",
              "                                                                                                   SimpleImputer(fill_value=0,\n",
              "                                                                                                                 strategy='constant')),\n",
              "                                                                                                  ('scaler',\n",
              "                                                                                                   StandardScaler())]),\n",
              "                                                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8e20>),\n",
              "                                                                                 ('cat',\n",
              "                                                                                  Pipeline(steps=[('imputer',\n",
              "                                                                                                   S...\n",
              "                                            'model': [GradientBoostingClassifier(),\n",
              "                                                      LogisticRegression(),\n",
              "                                                      KNeighborsClassifier(),\n",
              "                                                      SVC(), MLPClassifier(),\n",
              "                                                      AdaBoostClassifier(),\n",
              "                                                      RandomForestClassifier()],\n",
              "                                            'preprocessor__cat__onehot': [OneHotEncoder(handle_unknown='ignore',\n",
              "                                                                                        sparse_output=False),\n",
              "                                                                          TargetEncoder()],\n",
              "                                            'preprocessor__num__scaler': [StandardScaler(),\n",
              "                                                                          MinMaxScaler()],\n",
              "                                            'reducer': [SparsePCA()]}],\n",
              "                      verbose=1)"
            ],
            "text/html": [
              "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingRandomSearchCV(estimator=Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                                                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
              "                                                                   transformers=[(&#x27;num&#x27;,\n",
              "                                                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                                                   SimpleImputer(fill_value=0,\n",
              "                                                                                                                 strategy=&#x27;constant&#x27;)),\n",
              "                                                                                                  (&#x27;scaler&#x27;,\n",
              "                                                                                                   StandardScaler())]),\n",
              "                                                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8e20&gt;),\n",
              "                                                                                 (&#x27;cat&#x27;,\n",
              "                                                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                                                   S...\n",
              "                                            &#x27;model&#x27;: [GradientBoostingClassifier(),\n",
              "                                                      LogisticRegression(),\n",
              "                                                      KNeighborsClassifier(),\n",
              "                                                      SVC(), MLPClassifier(),\n",
              "                                                      AdaBoostClassifier(),\n",
              "                                                      RandomForestClassifier()],\n",
              "                                            &#x27;preprocessor__cat__onehot&#x27;: [OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                                                                        sparse_output=False),\n",
              "                                                                          TargetEncoder()],\n",
              "                                            &#x27;preprocessor__num__scaler&#x27;: [StandardScaler(),\n",
              "                                                                          MinMaxScaler()],\n",
              "                                            &#x27;reducer&#x27;: [SparsePCA()]}],\n",
              "                      verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-71\" type=\"checkbox\" ><label for=\"sk-estimator-id-71\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingRandomSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingRandomSearchCV(estimator=Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                                                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
              "                                                                   transformers=[(&#x27;num&#x27;,\n",
              "                                                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                                                   SimpleImputer(fill_value=0,\n",
              "                                                                                                                 strategy=&#x27;constant&#x27;)),\n",
              "                                                                                                  (&#x27;scaler&#x27;,\n",
              "                                                                                                   StandardScaler())]),\n",
              "                                                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8e20&gt;),\n",
              "                                                                                 (&#x27;cat&#x27;,\n",
              "                                                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                                                   S...\n",
              "                                            &#x27;model&#x27;: [GradientBoostingClassifier(),\n",
              "                                                      LogisticRegression(),\n",
              "                                                      KNeighborsClassifier(),\n",
              "                                                      SVC(), MLPClassifier(),\n",
              "                                                      AdaBoostClassifier(),\n",
              "                                                      RandomForestClassifier()],\n",
              "                                            &#x27;preprocessor__cat__onehot&#x27;: [OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                                                                        sparse_output=False),\n",
              "                                                                          TargetEncoder()],\n",
              "                                            &#x27;preprocessor__num__scaler&#x27;: [StandardScaler(),\n",
              "                                                                          MinMaxScaler()],\n",
              "                                            &#x27;reducer&#x27;: [SparsePCA()]}],\n",
              "                      verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-72\" type=\"checkbox\" ><label for=\"sk-estimator-id-72\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
              "                                   transformers=[(&#x27;num&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                   SimpleImputer(fill_value=0,\n",
              "                                                                                 strategy=&#x27;constant&#x27;)),\n",
              "                                                                  (&#x27;scaler&#x27;,\n",
              "                                                                   StandardScaler())]),\n",
              "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8e20&gt;),\n",
              "                                                 (&#x27;cat&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                   SimpleImputer(fill_value=&#x27;none&#x27;,\n",
              "                                                                                 strategy=&#x27;constant&#x27;)),\n",
              "                                                                  (&#x27;onehot&#x27;,\n",
              "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                                                                 sparse_output=False))]),\n",
              "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8f40&gt;)])),\n",
              "                (&#x27;reducer&#x27;, SparsePCA()),\n",
              "                (&#x27;feature_selection&#x27;,\n",
              "                 ConditionalVarianceThreshold(threshold=0.01)),\n",
              "                (&#x27;model&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-73\" type=\"checkbox\" ><label for=\"sk-estimator-id-73\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
              "                  transformers=[(&#x27;num&#x27;,\n",
              "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                  SimpleImputer(fill_value=0,\n",
              "                                                                strategy=&#x27;constant&#x27;)),\n",
              "                                                 (&#x27;scaler&#x27;, StandardScaler())]),\n",
              "                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8e20&gt;),\n",
              "                                (&#x27;cat&#x27;,\n",
              "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                  SimpleImputer(fill_value=&#x27;none&#x27;,\n",
              "                                                                strategy=&#x27;constant&#x27;)),\n",
              "                                                 (&#x27;onehot&#x27;,\n",
              "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                                                sparse_output=False))]),\n",
              "                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8f40&gt;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-74\" type=\"checkbox\" ><label for=\"sk-estimator-id-74\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8e20&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-75\" type=\"checkbox\" ><label for=\"sk-estimator-id-75\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(fill_value=0, strategy=&#x27;constant&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-76\" type=\"checkbox\" ><label for=\"sk-estimator-id-76\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-77\" type=\"checkbox\" ><label for=\"sk-estimator-id-77\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8f40&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-78\" type=\"checkbox\" ><label for=\"sk-estimator-id-78\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(fill_value=&#x27;none&#x27;, strategy=&#x27;constant&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-79\" type=\"checkbox\" ><label for=\"sk-estimator-id-79\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-80\" type=\"checkbox\" ><label for=\"sk-estimator-id-80\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre></pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-81\" type=\"checkbox\" ><label for=\"sk-estimator-id-81\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-82\" type=\"checkbox\" ><label for=\"sk-estimator-id-82\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SparsePCA</label><div class=\"sk-toggleable__content\"><pre>SparsePCA()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-83\" type=\"checkbox\" ><label for=\"sk-estimator-id-83\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ConditionalVarianceThreshold</label><div class=\"sk-toggleable__content\"><pre>ConditionalVarianceThreshold(threshold=0.01)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-84\" type=\"checkbox\" ><label for=\"sk-estimator-id-84\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('drop_columns', DropColumns()),\n",
              "                ('remove_duplicates', RemoveDuplicates()),\n",
              "                ('calculate_gas', CalculateEarnedGas()),\n",
              "                ('feature_engineering', FeatureEngineering())])"
            ],
            "text/html": [
              "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;drop_columns&#x27;, DropColumns()),\n",
              "                (&#x27;remove_duplicates&#x27;, RemoveDuplicates()),\n",
              "                (&#x27;calculate_gas&#x27;, CalculateEarnedGas()),\n",
              "                (&#x27;feature_engineering&#x27;, FeatureEngineering())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-85\" type=\"checkbox\" ><label for=\"sk-estimator-id-85\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;drop_columns&#x27;, DropColumns()),\n",
              "                (&#x27;remove_duplicates&#x27;, RemoveDuplicates()),\n",
              "                (&#x27;calculate_gas&#x27;, CalculateEarnedGas()),\n",
              "                (&#x27;feature_engineering&#x27;, FeatureEngineering())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-86\" type=\"checkbox\" ><label for=\"sk-estimator-id-86\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DropColumns</label><div class=\"sk-toggleable__content\"><pre>DropColumns()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-87\" type=\"checkbox\" ><label for=\"sk-estimator-id-87\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RemoveDuplicates</label><div class=\"sk-toggleable__content\"><pre>RemoveDuplicates()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-88\" type=\"checkbox\" ><label for=\"sk-estimator-id-88\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CalculateEarnedGas</label><div class=\"sk-toggleable__content\"><pre>CalculateEarnedGas()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-89\" type=\"checkbox\" ><label for=\"sk-estimator-id-89\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FeatureEngineering</label><div class=\"sk-toggleable__content\"><pre>FeatureEngineering()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HalvingRandomSearchCV(error_score='raise',\n",
              "                      estimator=Pipeline(steps=[('preprocessor',\n",
              "                                                 ColumnTransformer(remainder='passthrough',\n",
              "                                                                   transformers=[('num',\n",
              "                                                                                  Pipeline(steps=[('imputer',\n",
              "                                                                                                   SimpleImputer(fill_value=0,\n",
              "                                                                                                                 strategy='constant')),\n",
              "                                                                                                  ('scaler',\n",
              "                                                                                                   StandardScaler())]),\n",
              "                                                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8e20>),\n",
              "                                                                                 ('cat',\n",
              "                                                                                  Pipeline...\n",
              "                                                                                                   SimpleImputer(fill_value='none',\n",
              "                                                                                                                 strategy='constant')),\n",
              "                                                                                                  ('onehot',\n",
              "                                                                                                   OneHotEncoder(handle_unknown='ignore',\n",
              "                                                                                                                 sparse_output=False))]),\n",
              "                                                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8f40>)])),\n",
              "                                                ('model',\n",
              "                                                 SpectralClustering(random_state=0))]),\n",
              "                      n_jobs=-1,\n",
              "                      param_distributions=[{'init': ['random'],\n",
              "                                            'n_clusters': [5],\n",
              "                                            'n_init': [50]}],\n",
              "                      random_state=0, verbose=1)"
            ],
            "text/html": [
              "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingRandomSearchCV(error_score=&#x27;raise&#x27;,\n",
              "                      estimator=Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                                                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
              "                                                                   transformers=[(&#x27;num&#x27;,\n",
              "                                                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                                                   SimpleImputer(fill_value=0,\n",
              "                                                                                                                 strategy=&#x27;constant&#x27;)),\n",
              "                                                                                                  (&#x27;scaler&#x27;,\n",
              "                                                                                                   StandardScaler())]),\n",
              "                                                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8e20&gt;),\n",
              "                                                                                 (&#x27;cat&#x27;,\n",
              "                                                                                  Pipeline...\n",
              "                                                                                                   SimpleImputer(fill_value=&#x27;none&#x27;,\n",
              "                                                                                                                 strategy=&#x27;constant&#x27;)),\n",
              "                                                                                                  (&#x27;onehot&#x27;,\n",
              "                                                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                                                                                                 sparse_output=False))]),\n",
              "                                                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8f40&gt;)])),\n",
              "                                                (&#x27;model&#x27;,\n",
              "                                                 SpectralClustering(random_state=0))]),\n",
              "                      n_jobs=-1,\n",
              "                      param_distributions=[{&#x27;init&#x27;: [&#x27;random&#x27;],\n",
              "                                            &#x27;n_clusters&#x27;: [5],\n",
              "                                            &#x27;n_init&#x27;: [50]}],\n",
              "                      random_state=0, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-90\" type=\"checkbox\" ><label for=\"sk-estimator-id-90\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingRandomSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingRandomSearchCV(error_score=&#x27;raise&#x27;,\n",
              "                      estimator=Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                                                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
              "                                                                   transformers=[(&#x27;num&#x27;,\n",
              "                                                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                                                   SimpleImputer(fill_value=0,\n",
              "                                                                                                                 strategy=&#x27;constant&#x27;)),\n",
              "                                                                                                  (&#x27;scaler&#x27;,\n",
              "                                                                                                   StandardScaler())]),\n",
              "                                                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8e20&gt;),\n",
              "                                                                                 (&#x27;cat&#x27;,\n",
              "                                                                                  Pipeline...\n",
              "                                                                                                   SimpleImputer(fill_value=&#x27;none&#x27;,\n",
              "                                                                                                                 strategy=&#x27;constant&#x27;)),\n",
              "                                                                                                  (&#x27;onehot&#x27;,\n",
              "                                                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                                                                                                 sparse_output=False))]),\n",
              "                                                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8f40&gt;)])),\n",
              "                                                (&#x27;model&#x27;,\n",
              "                                                 SpectralClustering(random_state=0))]),\n",
              "                      n_jobs=-1,\n",
              "                      param_distributions=[{&#x27;init&#x27;: [&#x27;random&#x27;],\n",
              "                                            &#x27;n_clusters&#x27;: [5],\n",
              "                                            &#x27;n_init&#x27;: [50]}],\n",
              "                      random_state=0, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-91\" type=\"checkbox\" ><label for=\"sk-estimator-id-91\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
              "                                   transformers=[(&#x27;num&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                   SimpleImputer(fill_value=0,\n",
              "                                                                                 strategy=&#x27;constant&#x27;)),\n",
              "                                                                  (&#x27;scaler&#x27;,\n",
              "                                                                   StandardScaler())]),\n",
              "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8e20&gt;),\n",
              "                                                 (&#x27;cat&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                   SimpleImputer(fill_value=&#x27;none&#x27;,\n",
              "                                                                                 strategy=&#x27;constant&#x27;)),\n",
              "                                                                  (&#x27;onehot&#x27;,\n",
              "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                                                                 sparse_output=False))]),\n",
              "                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8f40&gt;)])),\n",
              "                (&#x27;model&#x27;, SpectralClustering(random_state=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-92\" type=\"checkbox\" ><label for=\"sk-estimator-id-92\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
              "                  transformers=[(&#x27;num&#x27;,\n",
              "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                  SimpleImputer(fill_value=0,\n",
              "                                                                strategy=&#x27;constant&#x27;)),\n",
              "                                                 (&#x27;scaler&#x27;, StandardScaler())]),\n",
              "                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8e20&gt;),\n",
              "                                (&#x27;cat&#x27;,\n",
              "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                  SimpleImputer(fill_value=&#x27;none&#x27;,\n",
              "                                                                strategy=&#x27;constant&#x27;)),\n",
              "                                                 (&#x27;onehot&#x27;,\n",
              "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
              "                                                                sparse_output=False))]),\n",
              "                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8f40&gt;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-93\" type=\"checkbox\" ><label for=\"sk-estimator-id-93\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8e20&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-94\" type=\"checkbox\" ><label for=\"sk-estimator-id-94\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(fill_value=0, strategy=&#x27;constant&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-95\" type=\"checkbox\" ><label for=\"sk-estimator-id-95\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-96\" type=\"checkbox\" ><label for=\"sk-estimator-id-96\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7dff52fa8f40&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-97\" type=\"checkbox\" ><label for=\"sk-estimator-id-97\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(fill_value=&#x27;none&#x27;, strategy=&#x27;constant&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-98\" type=\"checkbox\" ><label for=\"sk-estimator-id-98\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-99\" type=\"checkbox\" ><label for=\"sk-estimator-id-99\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre></pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-100\" type=\"checkbox\" ><label for=\"sk-estimator-id-100\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-101\" type=\"checkbox\" ><label for=\"sk-estimator-id-101\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SpectralClustering</label><div class=\"sk-toggleable__content\"><pre>SpectralClustering(random_state=0)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.impute import SimpleImputer\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, OrdinalEncoder, LabelEncoder, FunctionTransformer, PowerTransformer, QuantileTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, QuantileTransformer, Normalizer, PowerTransformer\n",
        "from sklearn.pipeline import clone\n",
        "\n",
        "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD, FastICA\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tune_sklearn import TuneSearchCV, TuneGridSearchCV\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from sklearn.feature_selection import RFE, SelectKBest, VarianceThreshold, f_classif\n",
        "\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "import time\n",
        "from sklearn.experimental import enable_halving_search_cv  # Needed to enable HalvingGridSearchCV and HalvingRandomSearchCV\n",
        "from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "\n",
        "preprocessing_pipeline_un= Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model',SpectralClustering(random_state=0) )  # Placeholder, actual model set in param_grid\n",
        "\n",
        "    # ('to_dense', CustomDense())\n",
        "])\n",
        "pipeline = ImbPipeline(steps=[\n",
        "\n",
        "    # ('sampling', ConditionalSampler(sampling_method=SMOTE())),\n",
        "    ('preprocessor', preprocessor),\n",
        "      # ('to_dense', CustomDense()),\n",
        "  #  ('to_dense', FunctionTransformer(to_dense)),\n",
        "     ('reducer',  SparsePCA()),\n",
        "     ('feature_selection', ConditionalVarianceThreshold(threshold=0.01)),\n",
        "\n",
        "    ('model', RandomForestClassifier())  # Placeholder, actual model set in param_grid\n",
        "])\n",
        "\n",
        "# Index(['hash', 'time_span', 'max_fee_per_gas', 'max_priority_fee_per_gas',\n",
        "#        'block_number', 'ofac_compliant', 'base_fee_per_gas', 'gas_limit',\n",
        "#        'gas_used', 'validator_pool', 'validator_name', 'n_transactions',\n",
        "#        'slot', 'relay', 'mevboost_value', 'builder', 'gas_earned', 'burnt',\n",
        "      #  'transaction_frequency', 'reverted'],\n",
        "# # Max fee per gas, max_priority_fee_per_gas\n",
        "# num_columns = df2.select_dtypes(include=np.number).columns\n",
        "# cat_columns = df2.select_dtypes(include=['object']).columns\n",
        "\n",
        "best_results = {}\n",
        "model_dir = '/content/drive/MyDrive/datasets/saved_models'\n",
        "# os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "selected='validator_name'\n",
        "selected= 'transaction_frequency'\n",
        "selected= 'ofac_compliant'\n",
        "\n",
        "\n",
        "def custom_clustering_score(estimator, X, y=None):\n",
        "    labels = estimator.fit_predict(X)\n",
        "    scores = {}\n",
        "    scores['silhouette'] = silhouette_score(X, labels)\n",
        "    scores['davies_bouldin'] = davies_bouldin_score(X, labels)\n",
        "    scores['calinski_harabasz'] = calinski_harabasz_score(X, labels)\n",
        "    combined_score = (scores['silhouette'] - scores['davies_bouldin'] + scores['calinski_harabasz'] / 3)\n",
        "    return combined_score\n",
        "\n",
        "  # Display the best results\n",
        "  # for name, result in best_results.items():\n",
        "#     print(f\"Best model with {name}: \", result['best_estimator'])\n",
        "#     print(f\"Best score with {name}: \", result['best_score'])\n",
        "# #   # Iterate over search strategies\n",
        "#   for name, search_cv in search_strategies:\n",
        "#       print(f\"Running {name}...\")\n",
        "#       start_time = time.time()\n",
        "#       search_cv.fit(X, y)\n",
        "#       end_time = time.time()\n",
        "#       elapsed_time = end_time - start_time\n",
        "\n",
        "#       best_results[name] = {\n",
        "#           # 'best_params': search_cv.best_params_,\n",
        "#           'best_estimator': search_cv.best_estimator_.named_steps['model'],\n",
        "#           'best_score': search_cv.best_score_,\n",
        "#           'time_spent': elapsed_time\n",
        "#       }\n",
        "\n",
        "# # Display the best results for each search strategy\n",
        "# for name, result in best_results.items():\n",
        "#     # print(f\"Best parameters found with {name}: \", result['best_params'])\n",
        "#     print(f\"Best model with {name}: \", result['best_estimator'])\n",
        "#     print(f\"Best score with {name}: \", result['best_score'])\n",
        "#     print(f\"Time spent with {name}: {result['time_spent']} seconds\")\n",
        "\n",
        "  # # Iterate over different CV strategies\n",
        "  # best_results = {}\n",
        "# for cv_name, cv_strategy in cv_strategies.items():\n",
        "#     random_search = TuneSearchCV(\n",
        "#         pipeline,\n",
        "#         param_distributions=param_grid,\n",
        "#         search_optimization=\"random\",\n",
        "#         max_iters=10,\n",
        "#         n_jobs=-1,\n",
        "#         verbose=1,\n",
        "#         error_score=\"raise\",\n",
        "#         time_budget_s=60,\n",
        "#         pipeline_auto_early_stop=True,\n",
        "#         cv=cv_strategy\n",
        "#     )\n",
        "\n",
        "#     # Fit the model using the current CV strategy\n",
        "#     random_search.fit(X, y)\n",
        "#     best_results[cv_name] = {\n",
        "#         'best_params': random_search.best_params_,\n",
        "#         'best_estimator': random_search.best_estimator_.named_steps['model'],\n",
        "#         'best_score': random_search.best_score_\n",
        "#     }\n",
        "\n",
        "# # Display the best results for each CV strategy\n",
        "# for cv_name, result in best_results.items():\n",
        "#     print(f\"Best parameters found with {cv_name}: \", result['best_params'])\n",
        "#     print(f\"Best model with {cv_name}: \", result['best_estimator'])\n",
        "#     print(f\"Best score with {cv_name}: \", result['best_score'])\n",
        "\n",
        "# 'transaction_frequency'\n",
        "\n",
        "param_grid = [\n",
        "    {\n",
        "        'reducer': [SparsePCA()],\n",
        "        'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n",
        "        'preprocessor__cat__onehot': [OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False), TargetEncoder()],\n",
        "        'feature_selection': [ConditionalVarianceThreshold(threshold=0.01)],\n",
        "        'model': [\n",
        "            GradientBoostingClassifier(),\n",
        "            LogisticRegression(), KNeighborsClassifier(), SVC(), MLPClassifier(),\n",
        "            AdaBoostClassifier(), RandomForestClassifier()\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "Halving = HalvingRandomSearchCV(\n",
        "            pipeline,\n",
        "            param_distributions=param_grid,\n",
        "            factor=3,\n",
        "            cv=5,\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )\n",
        "display(Halving)\n",
        "\n",
        "search = HalvingRandomSearchCV(\n",
        "            preprocessing_pipeline_un,\n",
        "            param_distributions=[{\n",
        "        'n_clusters': [5],\n",
        "        'init': ['random'],\n",
        "        'n_init': [50]\n",
        "    }],\n",
        "            # scoring=custom_clustering_score,\n",
        "            # search_optimization='hyperopt',\n",
        "            n_jobs=-1,\n",
        "            # time_budget_s=300,\n",
        "            cv=5,\n",
        "            refit=True,\n",
        "            verbose=1,\n",
        "            error_score=\"raise\",\n",
        "            random_state=0\n",
        "        )\n",
        "display(pipeline_cleaning)\n",
        "display(search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3znVz-BWFNLT"
      },
      "outputs": [],
      "source": [
        "# !pip install onnx onnxmltools onnxruntime\n",
        "# !pip install onnx skl2onnx onnxruntime scikit-learn imbalanced-learn category-encoders ray\n",
        "!pip list\n",
        "!pip freeze > requirements_colab.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPXc7YIyw2uR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "569520f5-1282-4bd9-c884-db0b446b1dab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "2024-06-24 10:08:52,744\tINFO worker.py:1724 -- Started a local Ray instance.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0d8084012523>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# from skl2onnx import convert_sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# from skl2onnx.common.data_types import FloatTensorType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# Initialize a lock object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mfile_write_lock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, labels, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1724\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1726\u001b[0;31m     connect(\n\u001b[0m\u001b[1;32m   1727\u001b[0m         \u001b[0m_global_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[0m_global_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(node, session_name, mode, log_to_driver, worker, driver_object_store_memory, job_id, namespace, job_config, runtime_env_hash, startup_token, ray_debugger_external, entrypoint, worker_launch_time_ms, worker_launched_time_ms)\u001b[0m\n\u001b[1;32m   2340\u001b[0m         \u001b[0mlogs_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logs_dir_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2342\u001b[0;31m     worker.core_worker = ray._raylet.CoreWorker(\n\u001b[0m\u001b[1;32m   2343\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplasma_store_socket_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.experimental import enable_halving_search_cv  # Needed to enable HalvingGridSearchCV and HalvingRandomSearchCV\n",
        "from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
        "from skopt import BayesSearchCV\n",
        "from tune_sklearn import TuneSearchCV, TuneGridSearchCV\n",
        "import concurrent.futures\n",
        "import joblib\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import threading\n",
        "import pickle\n",
        "import ray\n",
        "\n",
        "# from skl2onnx import convert_sklearn\n",
        "# from skl2onnx.common.data_types import FloatTensorType\n",
        "ray.init()\n",
        "# Initialize a lock object\n",
        "file_write_lock = threading.Lock()\n",
        "# Define your combinations, selected_dict, df2, num_columns, cat_columns, pipeline, and model_dir beforehand\n",
        "# combinations = ...\n",
        "# selected_dict = ...\n",
        "# df2 = ...\n",
        "# num_columns = ...\n",
        "# cat_columns = ...\n",
        "# pipeline = ...\n",
        "# model_dir = ...\n",
        "\n",
        "def process_combination(comb, selected = 'ofac_compliant'):\n",
        "    if not comb:\n",
        "        return\n",
        "\n",
        "    selected_features = set()\n",
        "    for group in comb:\n",
        "        selected_features.update(selected_dict[group])\n",
        "\n",
        "    selected_features = list(selected_features)\n",
        "    # print(selected_features)..\n",
        "\n",
        "    # X = df2[selected_features].head(10000)\n",
        "    # if selected in X.columns:\n",
        "    #   X = X.drop(columns=[selected])\n",
        "    # y = df2[selected].head(10000)\n",
        "\n",
        "\n",
        "    false_index = df2[df2[selected] == False].index[0]\n",
        "    start_index = false_index\n",
        "    end_index = start_index + 10000\n",
        "    end_index = min(end_index, len(df2))\n",
        "    selected_rows = df2.iloc[start_index:end_index]\n",
        "    X = selected_rows[selected_features]\n",
        "    if selected in X.columns:\n",
        "      X = X.drop(columns=[selected])\n",
        "\n",
        "    y = selected_rows[selected]\n",
        "    oversampler = RandomOverSampler(random_state=42)\n",
        "    X, y = oversampler.fit_resample(X, y)\n",
        "\n",
        "\n",
        "\n",
        "    print(len(y))\n",
        "    print(len(X))\n",
        "\n",
        "    param_grid = [\n",
        "        {\n",
        "            'reducer': [SparsePCA()],\n",
        "            'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n",
        "            'preprocessor__cat__onehot': [OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False), TargetEncoder()],\n",
        "            'feature_selection': [ConditionalVarianceThreshold(threshold=0.01)],\n",
        "            'model': [\n",
        "                GradientBoostingClassifier(),\n",
        "                LogisticRegression(), KNeighborsClassifier(), SVC(), MLPClassifier(),\n",
        "                AdaBoostClassifier(), RandomForestClassifier()\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    search_strategies = [\n",
        "        ('HalvingRandomSearchCV', HalvingRandomSearchCV(\n",
        "            pipeline,\n",
        "            param_distributions=param_grid,\n",
        "            factor=3,\n",
        "            cv=5,\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )),\n",
        "    ]\n",
        "\n",
        "    best_results = {}\n",
        "    for name, search_cv in search_strategies:\n",
        "        print(f\"Running {name} for combination: {comb}\")\n",
        "        start_time = time.time()\n",
        "        search_cv.fit(X, y)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        # model_filename = f\"{model_dir}/{name}_{str(comb)}.joblib\"\n",
        "\n",
        "        model_filename = f\"test{model_dir}/{name}_{str(comb)}.joblib\"\n",
        "\n",
        "        if hasattr(search_cv, 'best_estimator_'):\n",
        "            minio_client = Minio(param_s3_server, access_key=param_s3_access_key, secret_key=param_s3_secret_key, secure=True)\n",
        "            object_name = f'{param_s3_user_prefix}/vl-openlab/icos-naavre-demo/{name}_{comb}.pkl'  # Adjust the filename as needed\n",
        "            with file_write_lock:  # Lock the file writing operation\n",
        "              with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "                    temp_file.write(dill.dumps(search_cv.best_estimator_))\n",
        "              try:\n",
        "                  minio_client.fput_object(\n",
        "                      param_s3_bucket,\n",
        "                      object_name,\n",
        "                      temp_file.name\n",
        "                  )\n",
        "                  print(f\"Serialized model uploaded successfully to MinIO and replaced the existing file: {object_name}\")\n",
        "              except Exception as e:\n",
        "                  print(f\"Error uploading serialized model to MinIO: {e}\")\n",
        "\n",
        "        best_results[f\"{name}_{comb}\"] = {\n",
        "            'best_estimator': search_cv.best_estimator_.named_steps['model'],\n",
        "            'best_score': search_cv.best_score_,\n",
        "            'best_params': search_cv.best_params_,\n",
        "            'best_index': search_cv.best_index_,\n",
        "            'cv_results': search_cv.cv_results_,\n",
        "            'time_spent': elapsed_time\n",
        "        }\n",
        "    return best_results\n",
        "all_results = []\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    futures = {executor.submit(process_combination, comb): comb for comb in combinations}\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        comb = futures[future]\n",
        "        try:\n",
        "            result = future.result()\n",
        "            all_results.append(result)\n",
        "        except Exception as exc:\n",
        "            print(f'Combination {comb} generated an exception: {exc}')\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9bD7e046_Un"
      },
      "outputs": [],
      "source": [
        "# Assuming `all_results` has been populated by the previous code\n",
        "import csv\n",
        "\n",
        "def write_results_to_csv(results, filename):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        header_written = False\n",
        "\n",
        "        for result in results:\n",
        "            for model_name, details in result.items():\n",
        "                if not header_written:\n",
        "                    # Write header\n",
        "                    headers = [\"Model\", \"Best Params\", \"Best Score\", \"Cross-Validation Results\"]\n",
        "                    writer.writerow(headers)\n",
        "                    header_written = True\n",
        "\n",
        "                # Write model details\n",
        "                best_params = ', '.join([f\"{param}: {value}\" for param, value in details['best_params'].items()])\n",
        "                best_score = details['best_score']\n",
        "                cv_results = ', '.join([f\"{cv_key}: {cv_value}\" for cv_key, cv_value in details['best_cv'].items()])\n",
        "\n",
        "                row = [model_name, best_params, best_score, cv_results]\n",
        "                writer.writerow(row)\n",
        "# Write the results to a CSV file\n",
        "write_results_to_csv(all_results, '/content/drive/MyDrive/datasets/supervised.csv')\n",
        "# write_results_to_csv(outlier_results, '/content/drive/MyDrive/datasets/outlier_results.csv')\n",
        "\n",
        "\n",
        "print(\"Results written to 'results.csv'\")\n",
        "\n",
        "# def print_all_results(all_results):\n",
        "#     for comb, results in all_results.items():\n",
        "#         print(f\"Combination: {comb}\")\n",
        "#         if results:\n",
        "#             for strategy, result in results.items():\n",
        "#                 print(f\"  Strategy: {strategy}\")\n",
        "#                 print(f\"    Best Estimator: {result['best_estimator']}\")\n",
        "#                 print(f\"    Best Score: {result['best_score']}\")\n",
        "#                 print(f\"    Best Params: {json.dumps(result['best_params'], indent=4)}\")\n",
        "#                 print(f\"    Best Index: {result['best_index']}\")\n",
        "#                 print(f\"    Time Spent: {result['time_spent']} seconds\")\n",
        "#                 print(f\"    CV Results: {result['cv_results']}\")\n",
        "\n",
        "# # Call the function to print the results\n",
        "# print_all_results(all_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOM8CFviCpDS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA, FastICA, TruncatedSVD\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from tune_sklearn import TuneSearchCV\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "import joblib\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import threading\n",
        "import ray\n",
        "ray.init(ignore_reinit_error=True,log_to_driver=False)\n",
        "# Initialize a lock object\n",
        "file_write_lock = threading.Lock()\n",
        "\n",
        "def custom_clustering_score(estimator, X, y=None):\n",
        "    labels = estimator.fit_predict(X)\n",
        "    scores = {}\n",
        "    scores['silhouette'] = silhouette_score(X, labels)\n",
        "    scores['davies_bouldin'] = davies_bouldin_score(X, labels)\n",
        "    scores['calinski_harabasz'] = calinski_harabasz_score(X, labels)\n",
        "    combined_score = (scores['silhouette'] - scores['davies_bouldin'] + scores['calinski_harabasz'] / 3)\n",
        "    return combined_score\n",
        "\n",
        "def custom_outlier_score(estimator, X, y=None):\n",
        "    estimator.fit(X)\n",
        "    if isinstance(estimator, AutoencoderModel):\n",
        "        reconstructed = estimator.transform(X)\n",
        "        mse = np.mean(np.power(X - reconstructed, 2), axis=1)\n",
        "        threshold = np.percentile(mse, 95)\n",
        "        labels = np.where(mse > threshold, -1, 1)\n",
        "    else:\n",
        "        labels = estimator.fit_predict(X)\n",
        "    n_outliers = np.sum(labels == -1)\n",
        "    return n_outliers\n",
        "\n",
        "def build_autoencoder(input_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(64, activation='relu')(input_layer)\n",
        "    encoded = Dense(32, activation='relu')(encoded)\n",
        "    decoded = Dense(64, activation='relu')(encoded)\n",
        "    output_layer = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "    autoencoder = Model(input_layer, output_layer)\n",
        "    autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    return autoencoder\n",
        "\n",
        "class AutoencoderModel(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, input_dim, epochs=50, batch_size=32):\n",
        "        self.input_dim = input_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.autoencoder = build_autoencoder(input_dim)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.autoencoder.fit(X, X, epochs=self.epochs, batch_size=self.batch_size, shuffle=True)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.autoencoder.predict(X)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        self.fit(X)\n",
        "        reconstructed = self.transform(X)\n",
        "        mse = np.mean(np.power(X - reconstructed, 2), axis=1)\n",
        "        threshold = np.percentile(mse, 95)\n",
        "        return np.where(mse > threshold, -1, 1)\n",
        "\n",
        "clustering_models = {\n",
        "    'KMeans': (KMeans(random_state=0), {\n",
        "        'n_clusters': [5],\n",
        "        'init': ['random'],\n",
        "        'n_init': [50]\n",
        "    }),\n",
        "    'AgglomerativeClustering': (AgglomerativeClustering(), {\n",
        "        'n_clusters': [3],\n",
        "        'linkage': ['complete']\n",
        "    }),\n",
        "    'SpectralClustering': (SpectralClustering(random_state=0), {\n",
        "        'n_clusters': [5],\n",
        "        'eigen_solver': ['lobpcg'],\n",
        "        'affinity': ['nearest_neighbors']\n",
        "    }),\n",
        "    'GaussianMixture': (GaussianMixture(random_state=0), {\n",
        "        'n_components': [5],\n",
        "        'covariance_type': ['full']\n",
        "    })\n",
        "}\n",
        "\n",
        "outlier_models = {\n",
        "    'IsolationForest': (IsolationForest(random_state=0), {\n",
        "        'n_estimators': [200],\n",
        "        'contamination': [0.1]\n",
        "    }),\n",
        "    'OneClassSVM': (OneClassSVM(), {\n",
        "        'nu': [0.05],\n",
        "        'kernel': ['rbf'],\n",
        "        'gamma': ['scale']\n",
        "    }),\n",
        "    'Autoencoder': (AutoencoderModel(input_dim=30), {  # Adjust input_dim based on your dataset\n",
        "        'epochs': [50],\n",
        "        'batch_size': [32]\n",
        "    })\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define model_dir for saving models\n",
        "model_dir = '/content/drive/MyDrive/datasets/anomaly_models'\n",
        "\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "def evaluate_clustering(comb, selected_dict, preprocessing_pipeline, df2, model_dir):\n",
        "    if not comb:\n",
        "        return None\n",
        "    print(comb)\n",
        "    selected_features = set()\n",
        "    for group in comb:\n",
        "        selected_features.update(selected_dict[group])\n",
        "    selected_features = list(selected_features)\n",
        "    print(selected_features)\n",
        "    X = preprocessing_pipeline.fit_transform(df2[selected_features].head(10000))\n",
        "\n",
        "    best_clustering_results = {}\n",
        "    for model_name, (model, param_distributions) in clustering_models.items():\n",
        "        print(model_name)\n",
        "        search = TuneSearchCV(\n",
        "            model,\n",
        "            param_distributions=param_distributions,\n",
        "            n_trials=10,\n",
        "            scoring=custom_clustering_score,\n",
        "            search_optimization='hyperopt',\n",
        "            n_jobs=-1,\n",
        "            time_budget_s=300,\n",
        "            cv=5,\n",
        "            refit=True,\n",
        "            verbose=1,\n",
        "            error_score=\"raise\",\n",
        "            random_state=0\n",
        "        )\n",
        "        start_time = time.time()\n",
        "        search.fit(X)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        model_filename = f\"{model_dir}/{model_name}_{str(comb)}.joblib\"\n",
        "\n",
        "        if hasattr(search, 'best_estimator_'):\n",
        "            minio_client = Minio(param_s3_server, access_key=param_s3_access_key, secret_key=param_s3_secret_key, secure=True)\n",
        "            object_name = f'{param_s3_user_prefix}/vl-openlab/icos-naavre-demo/test_with_py.pkl'  # Adjust the filename as needed\n",
        "            with file_write_lock:  # Lock the file writing operation\n",
        "              with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "                    temp_file.write(search.best_estimator_)\n",
        "              try:\n",
        "                  minio_client.fput_object(\n",
        "                      param_s3_bucket,\n",
        "                      object_name,\n",
        "                      temp_file.name\n",
        "                  )\n",
        "                  print(f\"Serialized model uploaded successfully to MinIO and replaced the existing file: {object_name}\")\n",
        "              except Exception as e:\n",
        "                  print(f\"Error uploading serialized model to MinIO: {e}\")\n",
        "\n",
        "            best_clustering_results[model_name] = {\n",
        "                'best_params': search.best_params_,\n",
        "                'best_score': search.best_score_,\n",
        "                'best_cv': search.cv_results_,\n",
        "                'estimator': search.best_estimator_,\n",
        "                'time': elapsed_time\n",
        "            }\n",
        "    return best_clustering_results\n",
        "\n",
        "def evaluate_outliers(comb, selected_dict, preprocessing_pipeline, df2, model_dir):\n",
        "    if not comb:\n",
        "        return None\n",
        "    print(comb)\n",
        "    selected_features = set()\n",
        "    for group in comb:\n",
        "        selected_features.update(selected_dict[group])\n",
        "    selected_features = list(selected_features)\n",
        "    print(selected_features)\n",
        "    X = preprocessing_pipeline.fit_transform(df2[selected_features].head(10000))\n",
        "\n",
        "    best_outlier_results = {}\n",
        "    for model_name, (model, param_distributions) in outlier_models.items():\n",
        "\n",
        "        if model_name == 'Autoencoder':\n",
        "            model = AutoencoderModel(input_dim=X.shape[1])  # Update input_dim dynamically\n",
        "            # param_distributions['input_dim'] = [X.shape[1]]\n",
        "\n",
        "        search = TuneSearchCV(\n",
        "            model,\n",
        "            param_distributions=param_distributions,\n",
        "            scoring=custom_outlier_score,\n",
        "            cv=5,\n",
        "            search_optimization='hyperopt',\n",
        "            n_jobs=-1,\n",
        "            time_budget_s=300,\n",
        "            refit=True,\n",
        "            verbose=1,\n",
        "            error_score=\"raise\",\n",
        "            random_state=0\n",
        "        )\n",
        "        start_time = time.time()\n",
        "        search.fit(X)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        model_filename = f\"{model_dir}/{model_name}_{str(comb)}.joblib\"\n",
        "\n",
        "        if hasattr(search, 'best_params_'):\n",
        "            with file_write_lock:  # Lock the file writing operation\n",
        "              joblib.dump(search.best_estimator_, model_filename)\n",
        "\n",
        "            best_outlier_results[model_name] = {\n",
        "                'best_params': search.best_params_,\n",
        "                'best_score': search.best_score_,\n",
        "                'best_cv': search.cv_results_,\n",
        "                'estimator': search.best_estimator_,\n",
        "                'time': elapsed_time\n",
        "            }\n",
        "    return best_outlier_results\n",
        "\n",
        "# Use ThreadPoolExecutor to parallelize the evaluation for clustering\n",
        "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
        "    futures = [executor.submit(evaluate_clustering, comb, selected_dict, preprocessing_pipeline_un, df2, model_dir) for comb in combinations]\n",
        "    clustering_results = [future.result() for future in futures]\n",
        "\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrdtuDRrFGx4"
      },
      "outputs": [],
      "source": [
        "ray.init(ignore_reinit_error=True,log_to_driver=False)\n",
        "# Use ThreadPoolExecutor to parallelize the evaluation for outliers\n",
        "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
        "    futures = [executor.submit(evaluate_outliers, comb, selected_dict, preprocessing_pipeline, df2, model_dir) for comb in combinations if comb == combinations[-1]]\n",
        "    outlier_results = [future.result() for future in futures]\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3ZT4HqeDAi-"
      },
      "outputs": [],
      "source": [
        "print(len(clustering_results), len(outlier_results))\n",
        "# print(clustering_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqT2MRy9IucO"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def write_results_to_csv(results, filename):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        header_written = False\n",
        "\n",
        "        for result in results:\n",
        "            for model_name, details in result.items():\n",
        "                if not header_written:\n",
        "                    # Write header\n",
        "                    headers = [\"Model\", \"Best Params\", \"Best Score\", \"Cross-Validation Results\"]\n",
        "                    writer.writerow(headers)\n",
        "                    header_written = True\n",
        "\n",
        "                # Write model details\n",
        "                best_params = ', '.join([f\"{param}: {value}\" for param, value in details['best_params'].items()])\n",
        "                best_score = details['best_score']\n",
        "                cv_results = ', '.join([f\"{cv_key}: {cv_value}\" for cv_key, cv_value in details['best_cv'].items()])\n",
        "\n",
        "                row = [model_name, best_params, best_score, cv_results]\n",
        "                writer.writerow(row)\n",
        "# Write the results to a CSV file\n",
        "write_results_to_csv(clustering_results, '/content/drive/MyDrive/datasets/cluster_results.csv')\n",
        "write_results_to_csv(outlier_results, '/content/drive/MyDrive/datasets/outlier_results.csv')\n",
        "\n",
        "\n",
        "print(\"Results written to 'results.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ldndo03kWgu"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# combinations = combinations[-1]\n",
        "# selected= 'mevboost_value'\n",
        "# selected= 'transaction_frequency'\n",
        "# selected= 'reverted'\n",
        "# selected= 'validator_pool'\n",
        "for comb in combinations:\n",
        "  # if comb != combinations[-1]:\n",
        "  #     continue\n",
        "  if not comb:\n",
        "      continue\n",
        "  # print(comb)\n",
        "  selected_features = set()\n",
        "  for group in comb:\n",
        "      selected_features.update(selected_dict[group])\n",
        "\n",
        "  selected_features = list(selected_features)\n",
        "  print(selected_features)\n",
        "  for selected in ['ofac_compliant']:\n",
        "  # for selected in ['ofac_compliant']:\n",
        "\n",
        "    # selected= 'ofac_compliant''\n",
        "    X = df2[selected_features].head(10000)\n",
        "    if selected in X.columns:\n",
        "      X = X.drop(columns=[selected])\n",
        "    y = df2[selected].head(10000)\n",
        "\n",
        "    if selected == 'ofac_compliant':\n",
        "      # Step 1: Find the index of the first occurrence of False in 'ofac_compliant'\n",
        "      false_index = df2[df2[selected] == False].index[0]\n",
        "\n",
        "      # Step 2: Select the next 1000 rows from the false_index\n",
        "      start_index = false_index{frozenset({'general', 'compliant', 'amount', 'monetary', 'mev'}): 'AgglomerativeClustering', frozenset({'general', 'amount', 'compliant'}): 'HalvingRandomSearchCV', frozenset({'general', 'amount', 'compliant', 'mev'}): 'HalvingRandomSearchCV', frozenset({'general', 'amount', 'mev'}): 'HalvingRandomSearchCV', frozenset({'general', 'compliant', 'mev'}): 'HalvingRandomSearchCV', frozenset({'general', 'amount', 'monetary', 'compliant'}): 'HalvingRandomSearchCV', frozenset({'general', 'amount', 'monetary'}): 'HalvingRandomSearchCV', frozenset({'general', 'monetary', 'compliant'}): 'HalvingRandomSearchCV', frozenset({'general', 'amount', 'monetary', 'mev'}): 'HalvingRandomSearchCV', frozenset({'general', 'compliant', 'monetary', 'mev'}): 'HalvingRandomSearchCV', frozenset({'general', 'monetary', 'mev'}): 'HalvingRandomSearchCV', frozenset({'amount', 'compliant', 'mev'}): 'HalvingRandomSearchCV', frozenset({'amount', 'monetary', 'compliant'}): 'HalvingRandomSearchCV', frozenset({'compliant', 'amount', 'monetary', 'mev'}): 'HalvingRandomSearchCV', frozenset({'amount', 'monetary', 'mev'}): 'HalvingRandomSearchCV', frozenset({'compliant', 'monetary', 'mev'}): 'HalvingRandomSearchCV', frozenset({'model'}): 'best', frozenset({'general', 'amount', 'HalvingRandomSearchCV_{compliant', 'monetary', 'mev'}): 'test', frozenset({'with_pyHalvingRandomSearchCV'}): 'test'}\n",
        "      end_index = start_index + 10000\n",
        "\n",
        "      # Ensure we don't go out of bounds\n",
        "      end_index = min(end_index, len(df2))\n",
        "\n",
        "      # Select the rows\n",
        "      selected_rows = df2.iloc[start_index:end_index]\n",
        "\n",
        "      # Drop the 'ofac_compliant' column and separate the target column\n",
        "      X = selected_rows.drop(columns=[selected])\n",
        "      y = selected_rows[selected]\n",
        "\n",
        "      # RandomOverSampler(random_state=42)  # or\n",
        "      oversampler = RandomOverSampler(random_state=42)\n",
        "      X, y = oversampler.fit_resample(X, y)\n",
        "\n",
        "    elif selected in num_columns:\n",
        "\n",
        "      # Function to categorize values\n",
        "      def categorize(value, low_threshold, high_threshold):\n",
        "          if value <= low_threshold:\n",
        "              return 'Low'\n",
        "          elif value <= high_threshold:\n",
        "              return 'Medium'\n",
        "          else:\n",
        "              return 'High'\n",
        "\n",
        "      # Determine quantiles\n",
        "      low_threshold = y.quantile(0.33)\n",
        "      high_threshold = y.quantile(0.67)\n",
        "\n",
        "      # Apply categorization\n",
        "\n",
        "      y = y.apply(categorize, args=(low_threshold, high_threshold))\n",
        "\n",
        "      encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
        "      y = encoder.fit_transform(y.values.reshape(-1, 1)).ravel()\n",
        "\n",
        "      # y = pd.cut(y, bins=3, labels=['low', 'medium', 'high'])\n",
        "    elif selected in cat_columns:\n",
        "      label_encoder = LabelEncoder()\n",
        "      y = label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(len(y))\n",
        "    print(len(X))\n",
        "    # y = pipeline.named_steps['preprocessor'].transform(y)\n",
        "\n",
        "    # for nums\n",
        "    # y = pd.cut(y, bins=3, labels=['low', 'medium', 'high'])\n",
        "\n",
        "\n",
        "    # # Split the data into features and target\n",
        "    # X = df.drop(columns=['ofac_compliant']).head(1000)\n",
        "    # y = df['ofac_compliant'].head(1000)\n",
        "\n",
        "    param_grid = [\n",
        "        # 'model__n_estimators': [100, 200],\n",
        "        #  'model__min_samples_split': [2, 5, 10],\n",
        "        {\n",
        "            # 'reducer': [PCA(), TruncatedSVD()],\n",
        "            'reducer': [SparsePCA()],\n",
        "\n",
        "            # 'reducer__n_components': [0.95, 0.5, 1],\n",
        "            # 'reducer__whiten': [True, False],\n",
        "        # 'preprocessor__num__pca':[ PCA()],\n",
        "        #  'preprocessor__num__pca__n_components': [0.95, 0.5, 1],\n",
        "        #    'preprocessor__num__pca__whiten': [True, False],\n",
        "        # 'preprocessor__num__pca__n_oversamples': [10, 20],\n",
        "    #  'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n",
        "    #     'preprocessor__cat__onehot':[OneHotEncoder(handle_unknown=\"ignore\"), TargetEncoder()],\n",
        "    #     'model': [AdaBoostClassifier(), RandomForestClassifier()]\n",
        "    #     },\n",
        "    #      {\n",
        "        'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n",
        "        'preprocessor__cat__onehot':[OneHotEncoder(handle_unknown=\"ignore\"), TargetEncoder()],\n",
        "            # 'reducer': [ TruncatedSVD()],\n",
        "            # 'reducer__n_components': [2, 5, 10],\n",
        "             'feature_selection': [ConditionalVarianceThreshold(threshold=0.01)],\n",
        "            # 'model': [AdaBoostClassifier(), RandomForestClassifier()]\n",
        "        'model': [GradientBoostingClassifier(),\n",
        "                  LogisticRegression(), KNeighborsClassifier(), SVC(), MLPClassifier(),\n",
        "                 AdaBoostClassifier(), RandomForestClassifier()]\n",
        "            }\n",
        "\n",
        "    ]\n",
        "    cv_strategies = {\n",
        "        'kf': KFold(n_splits=5, shuffle=True, random_state=42),\n",
        "        'loo': LeaveOneOut()\n",
        "    }\n",
        "\n",
        "    import time\n",
        "    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "    from sklearn.experimental import enable_halving_search_cv  # Needed to enable HalvingGridSearchCV and HalvingRandomSearchCV\n",
        "    from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
        "    from skopt import BayesSearchCV\n",
        "    from tune_sklearn import TuneSearchCV, TuneGridSearchCV\n",
        "\n",
        "\n",
        "    # List of search strategies instantiated before the loop\n",
        "    search_strategies = [\n",
        "\n",
        "    #      ('BayesianOptimization', BayesSearchCV(\n",
        "    #         pipeline,\n",
        "    #         search_spaces=\n",
        "    #         {\n",
        "    #     # 'reducer__n_components': Real(0.5, 1.0, prior='uniform'),\n",
        "    #     # 'reducer__whiten': Categorical([True, False]),\n",
        "    #     'preprocessor__num__scaler': Categorical([StandardScaler(), MinMaxScaler()]),\n",
        "    #     'preprocessor__cat__onehot': Categorical([OneHotEncoder(handle_unknown=\"ignore\"), TargetEncoder()]),\n",
        "    #     'model': Categorical([RandomForestClassifier()]),\n",
        "    #     'model__n_estimators': Integer(50, 200),\n",
        "    #     'model__max_depth': Integer(1, 10)\n",
        "    # },\n",
        "    #         n_iter=10,\n",
        "    #         cv=5,\n",
        "    #         n_jobs=-1,\n",
        "    #         verbose=1\n",
        "    #     )),\n",
        "        # ('GridSearchCV', GridSearchCV(\n",
        "        #     pipeline,\n",
        "        #     param_grid=param_grid,\n",
        "        #     cv=5,\n",
        "        #     n_jobs=-1,\n",
        "        #     verbose=1\n",
        "        # )),\n",
        "        # ('RandomizedSearchCV', RandomizedSearchCV(\n",
        "        #     pipeline,\n",
        "        #     param_distributions=param_grid,\n",
        "        #     n_iter=5,\n",
        "        #     cv=5,\n",
        "        #     n_jobs=-1,\n",
        "        #     verbose=1\n",
        "        # )),\n",
        "        # ('HalvingGridSearchCV', HalvingGridSearchCV(\n",
        "        #     pipeline,\n",
        "        #     param_grid=param_grid,\n",
        "        #     factor=2,\n",
        "        #     cv=5,\n",
        "        #     n_jobs=-1,\n",
        "        #     verbose=1\n",
        "        # )),\n",
        "        ('HalvingRandomSearchCV', HalvingRandomSearchCV(\n",
        "            pipeline,\n",
        "            param_distributions=param_grid,\n",
        "            factor=3,\n",
        "            cv=5,\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )),\n",
        "\n",
        "        # ('TuneGridSearchCV', TuneGridSearchCV(\n",
        "        #     pipeline,\n",
        "        #     param_grid=param_grid,\n",
        "        #     max_iters=10,\n",
        "        #     n_jobs=-1,\n",
        "        #     verbose=1\n",
        "        # )),\n",
        "        # ('TuneSearchCV', TuneSearchCV(\n",
        "        #     pipeline,\n",
        "        #     param_distributions=param_grid,\n",
        "        #     search_optimization=\"random\",\n",
        "        #     max_iters=10,\n",
        "        #     n_jobs=-1,\n",
        "        #     verbose=1,\n",
        "        #     error_score=\"raise\",\n",
        "        #     time_budget_s=60,\n",
        "        #     pipeline_auto_early_stop=True,\n",
        "        #     cv=5  # Adjust this as needed\n",
        "        # ))\n",
        "    ]\n",
        "\n",
        "    for name, search_cv in search_strategies:\n",
        "        print(f\"Running {name} for combination: {comb}\")\n",
        "        start_time = time.time()\n",
        "        search_cv.fit(X, y)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        model_filename = f\"{model_dir}/{name}_{str(comb)}.joblib\"\n",
        "        joblib.dump(search_cv.best_estimator_, model_filename)\n",
        "\n",
        "        best_results[f\"{name}_{comb}\"] = {\n",
        "            'best_estimator': search_cv.best_estimator_.named_steps['model'],\n",
        "            'best_score': search_cv.best_score_,\n",
        "            'best_params': search_cv.best_params_,\n",
        "            'best_index': search_cv.best_index_,\n",
        "            'cv_results': search_cv.cv_results_,\n",
        "            'time_spent': elapsed_time\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8IFpln9_v1b"
      },
      "outputs": [],
      "source": [
        "df2.shape\n",
        "# X = preprocessing_pipeline.fit_transform(df2.head(10))\n",
        "# Determine the number of CPU cores\n",
        "num_cores = os.cpu_count()\n",
        "\n",
        "# Set the number of workers; for CPU-bound tasks, it's usually best to use the number of cores\n",
        "max_workers = num_cores\n",
        "print(max_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eST2zZB2AqAB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA, FastICA, TruncatedSVD\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from tune_sklearn import TuneSearchCV\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, silhouette_score, davies_bouldin_score, calinski_harabasz_score, mutual_info_score\n",
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "import joblib\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import threading\n",
        "\n",
        "# Initialize a lock object\n",
        "file_write_lock = threading.Lock()\n",
        "\n",
        "def custom_clustering_score(estimator, X, y=None):\n",
        "    labels = estimator.fit_predict(X)\n",
        "    scores = {}\n",
        "    scores['silhouette'] = silhouette_score(X, labels)\n",
        "    scores['davies_bouldin'] = davies_bouldin_score(X, labels)\n",
        "    scores['calinski_harabasz'] = calinski_harabasz_score(X, labels)\n",
        "    combined_score = (scores['silhouette'] - scores['davies_bouldin'] + scores['calinski_harabasz'] / 3)\n",
        "    return combined_score\n",
        "\n",
        "def custom_outlier_score(estimator, X, y=None):\n",
        "    estimator.fit(X)\n",
        "    if isinstance(estimator, AutoencoderModel):\n",
        "        reconstructed = estimator.transform(X)\n",
        "        mse = np.mean(np.power(X - reconstructed, 2), axis=1)\n",
        "        threshold = np.percentile(mse, 95)\n",
        "        labels = np.where(mse > threshold, -1, 1)\n",
        "    else:\n",
        "        labels = estimator.fit_predict(X)\n",
        "    n_outliers = np.sum(labels == -1)\n",
        "    return n_outliers\n",
        "\n",
        "def build_autoencoder(input_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(64, activation='relu')(input_layer)\n",
        "    encoded = Dense(32, activation='relu')(encoded)\n",
        "    decoded = Dense(64, activation='relu')(encoded)\n",
        "    output_layer = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "    autoencoder = Model(input_layer, output_layer)\n",
        "    autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    return autoencoder\n",
        "\n",
        "class AutoencoderModel(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, input_dim, epochs=50, batch_size=32):\n",
        "        self.input_dim = input_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.autoencoder = build_autoencoder(input_dim)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.autoencoder.fit(X, X, epochs=self.epochs, batch_size=self.batch_size, shuffle=True)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.autoencoder.predict(X)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        self.fit(X)\n",
        "        reconstructed = self.transform(X)\n",
        "        mse = np.mean(np.power(X - reconstructed, 2), axis=1)\n",
        "        threshold = np.percentile(mse, 95)\n",
        "        return np.where(mse > threshold, -1, 1)\n",
        "\n",
        "clustering_models = {\n",
        "    'KMeans': (KMeans(random_state=0), {\n",
        "        'n_clusters': [5],\n",
        "        'init': ['random'],\n",
        "        'n_init': [50]\n",
        "    }),\n",
        "    'AgglomerativeClustering': (AgglomerativeClustering(), {\n",
        "        'n_clusters': [3],\n",
        "        'linkage': ['complete']\n",
        "    }),\n",
        "    'SpectralClustering': (SpectralClustering(random_state=0), {\n",
        "        'n_clusters': [5],\n",
        "        'eigen_solver': ['lobpcg'],\n",
        "        'affinity': ['nearest_neighbors']\n",
        "    }),\n",
        "    'GaussianMixture': (GaussianMixture(random_state=0), {\n",
        "        'n_components': [5],\n",
        "        'covariance_type': ['full']\n",
        "    })\n",
        "}\n",
        "\n",
        "outlier_models = {\n",
        "    'IsolationForest': (IsolationForest(random_state=0), {\n",
        "        'n_estimators': [200],\n",
        "        'contamination': [0.1]\n",
        "    }),\n",
        "    'OneClassSVM': (OneClassSVM(), {\n",
        "        'nu': [0.05],\n",
        "        'kernel': ['rbf'],\n",
        "        'gamma': ['scale']\n",
        "    }),\n",
        "    'Autoencoder': (AutoencoderModel(input_dim=30), {  # Adjust input_dim based on your dataset\n",
        "        'epochs': [50],\n",
        "        'batch_size': [32]\n",
        "    })\n",
        "}\n",
        "\n",
        "def evaluate_combination(comb, selected_dict, preprocessing_pipeline, df2, model_dir):\n",
        "    if not comb:\n",
        "        return None, None\n",
        "    print(comb)\n",
        "    selected_features = set()\n",
        "    for group in comb:\n",
        "        selected_features.update(selected_dict[group])\n",
        "    selected_features = list(selected_features)\n",
        "    X = preprocessing_pipeline.fit_transform(df2[selected_features].head(100))\n",
        "\n",
        "    best_clustering_results = {}\n",
        "    for model_name, (model, param_distributions) in clustering_models.items():\n",
        "        print(model_name)\n",
        "        search = TuneSearchCV(\n",
        "            model,\n",
        "            param_distributions=param_distributions,\n",
        "            n_trials=10,\n",
        "            scoring=custom_clustering_score,\n",
        "            search_optimization='hyperopt',\n",
        "            n_jobs=-1,\n",
        "            time_budget_s=300,\n",
        "            cv=5,\n",
        "            refit=True,\n",
        "            verbose=1,\n",
        "            error_score=\"raise\",\n",
        "            random_state=0\n",
        "        )\n",
        "        start_time = time.time()\n",
        "        search.fit(X)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        if hasattr(search, 'best_params_'):\n",
        "            best_clustering_results[model_name] = {\n",
        "                'best_params': search.best_params_,\n",
        "                'best_score': search.best_score_,\n",
        "                'best_cv': search.cv_results_,\n",
        "                'estimator': search.best_estimator_,\n",
        "                'time': elapsed_time\n",
        "            }\n",
        "\n",
        "    best_outlier_results = {}\n",
        "    for model_name, (model, param_distributions) in outlier_models.items():\n",
        "        search = TuneSearchCV(\n",
        "            model,\n",
        "            param_distributions=param_distributions,\n",
        "            scoring=custom_outlier_score,\n",
        "            cv=5,\n",
        "            search_optimization='hyperopt',\n",
        "            n_jobs=-1,\n",
        "            time_budget_s=300,\n",
        "            refit=True,\n",
        "            verbose=1,\n",
        "            error_score=\"raise\",\n",
        "            random_state=0\n",
        "        )\n",
        "        start_time = time.time()\n",
        "        search.fit(X)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        if hasattr(search, 'best_params_'):\n",
        "            best_outlier_results[model_name] = {\n",
        "                'best_params': search.best_params_,\n",
        "                'best_score': search.best_score_,\n",
        "                'best_cv': search.cv_results_,\n",
        "                'estimator': search.best_estimator_,\n",
        "                'time': elapsed_time\n",
        "            }\n",
        "\n",
        "    return best_clustering_results, best_outlier_results\n",
        "\n",
        "# Use ThreadPoolExecutor to parallelize the evaluation\n",
        "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
        "    futures = [executor.submit(evaluate_combination, comb, selected_dict, preprocessing_pipeline, df2, model_dir) for comb in combinations]\n",
        "    results = [future.result() for future in futures]\n",
        "\n",
        "# Process results\n",
        "for i, comb in enumerate(combinations):\n",
        "    if comb != combinations[-1]:\n",
        "      continue\n",
        "    best_clustering_results, best_outlier_results = results[i]\n",
        "\n",
        "    if best_clustering_results:\n",
        "        best_clustering_model_name = None\n",
        "        best_clustering_score = -float('inf')\n",
        "        for model_name, result in best_clustering_results.items():\n",
        "            if result['best_score'] > best_clustering_score:\n",
        "                best_clustering_score = result['best_score']\n",
        "                best_clustering_model_name = model_name\n",
        "                model_filename = f\"{model_dir}/{model_name}_{str(comb)}.joblib\"\n",
        "                with file_write_lock:  # Lock the file writing operation\n",
        "                    joblib.dump(result['estimator'], model_filename)\n",
        "        if best_clustering_model_name:\n",
        "            print(\"Best Clustering Model:\")\n",
        "            print(f\"Model: {best_clustering_model_name}\")\n",
        "            print(\"Best Params:\", best_clustering_results[best_clustering_model_name]['best_params'])\n",
        "            print(\"Best Score:\", best_clustering_results[best_clustering_model_name]['best_score'])\n",
        "            print(\"Best cv:\", best_clustering_results[best_clustering_model_name]['best_cv'])\n",
        "\n",
        "    if best_outlier_results:\n",
        "        best_outlier_model_name = None\n",
        "        best_outlier_score = -float('inf')\n",
        "        for model_name, result in best_outlier_results.items():\n",
        "            if result['best_score'] > best_outlier_score:\n",
        "                best_outlier_score = result['best_score']\n",
        "                best_outlier_model_name = model_name\n",
        "                model_filename = f\"{model_dir}/{model_name}_{str(comb)}.joblib\"\n",
        "                with file_write_lock:  # Lock the file writing operation\n",
        "                    joblib.dump(result['estimator'], model_filename)\n",
        "        if best_outlier_model_name:\n",
        "            print(\"Best Outlier Model:\")\n",
        "            print(f\"Model: {best_outlier_model_name}\")\n",
        "            print(\"Best Params:\", best_outlier_results[best_outlier_model_name]['best_params'])\n",
        "            print(\"Best Score:\", best_outlier_results[best_outlier_model_name]['best_score'])\n",
        "            print(\"Best cv:\", best_outlier_results[best_outlier_model_name]['best_cv'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUO29_pkVM9Z"
      },
      "outputs": [],
      "source": [
        "print(best_clustering_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llaFWaBOzTbc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA, FastICA, TruncatedSVD\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from tune_sklearn import TuneSearchCV\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, silhouette_score, davies_bouldin_score, calinski_harabasz_score, mutual_info_score\n",
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "# Example data\n",
        "\n",
        "\n",
        "def custom_clustering_score(estimator, X, y=None):\n",
        "    labels = estimator.fit_predict(X)\n",
        "    scores = {}\n",
        "    scores['silhouette'] = silhouette_score(X, labels)\n",
        "    scores['davies_bouldin'] = davies_bouldin_score(X, labels)\n",
        "    scores['calinski_harabasz'] = calinski_harabasz_score(X, labels)\n",
        "    # scores['dunn_index'] = dunn_index(X, labels)  # Assuming you have a function to calculate Dunn Index\n",
        "    # scores['mutual_information'] = mutual_info_score(None, labels)  # Mutual information doesn't need true labels\n",
        "\n",
        "    # Combine the scores (adjust weights as necessary)\n",
        "    combined_score = (scores['silhouette'] - scores['davies_bouldin'] + scores['calinski_harabasz'] /3)\n",
        "    return combined_score\n",
        "\n",
        "# Custom scoring function for outlier detection\n",
        "# def custom_outlier_score(estimator, X, y=None):\n",
        "def custom_outlier_score(estimator, X, y=None):\n",
        "    # Fit the model\n",
        "    estimator.fit(X)\n",
        "\n",
        "    # If the estimator is an autoencoder, calculate the reconstruction error\n",
        "    if isinstance(estimator, AutoencoderModel):\n",
        "        reconstructed = estimator.transform(X)\n",
        "        mse = np.mean(np.power(X - reconstructed, 2), axis=1)\n",
        "        # Using MSE as a proxy for outlier scores, converting it to binary labels\n",
        "        threshold = np.percentile(mse, 95)  # adjust threshold as needed\n",
        "        labels = np.where(mse > threshold, -1, 1)\n",
        "    else:\n",
        "        # For other models, use the fit_predict method to get the labels\n",
        "        labels = estimator.fit_predict(X)\n",
        "\n",
        "    # Calculate the number of outliers detected\n",
        "    n_outliers = np.sum(labels == -1)\n",
        "\n",
        "    # Return the number of outliers as the score\n",
        "    return n_outliers\n",
        "\n",
        "def build_autoencoder(input_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(64, activation='relu')(input_layer)\n",
        "    encoded = Dense(32, activation='relu')(encoded)\n",
        "    decoded = Dense(64, activation='relu')(encoded)\n",
        "    output_layer = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "    autoencoder = Model(input_layer, output_layer)\n",
        "    autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    return autoencoder\n",
        "\n",
        "class AutoencoderModel(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, input_dim, epochs=50, batch_size=32):\n",
        "        self.input_dim = input_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.autoencoder = build_autoencoder(input_dim)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.autoencoder.fit(X, X, epochs=self.epochs, batch_size=self.batch_size, shuffle=True)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.autoencoder.predict(X)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        self.fit(X)\n",
        "        reconstructed = self.transform(X)\n",
        "        mse = np.mean(np.power(X - reconstructed, 2), axis=1)\n",
        "        # Using MSE as a proxy for outlier scores, converting it to binary labels\n",
        "        threshold = np.percentile(mse, 95)  # adjust threshold as needed\n",
        "        return np.where(mse > threshold, -1, 1)\n",
        "\n",
        "clustering_models = {\n",
        "    'KMeans': (KMeans(random_state=0), {\n",
        "        'n_clusters': [5],\n",
        "        'init': ['random'],\n",
        "        'n_init': [50]\n",
        "    }),\n",
        "    'AgglomerativeClustering': (AgglomerativeClustering(), {\n",
        "        'n_clusters': [3],\n",
        "        'linkage': ['complete']\n",
        "    }),\n",
        "    'SpectralClustering': (SpectralClustering(random_state=0), {\n",
        "        'n_clusters': [5],\n",
        "        'eigen_solver': ['lobpcg'],\n",
        "        'affinity': ['nearest_neighbors']\n",
        "    }),\n",
        "    'GaussianMixture': (GaussianMixture(random_state=0), {\n",
        "        'n_components': [5],\n",
        "        'covariance_type': ['full']\n",
        "    })\n",
        "}\n",
        "\n",
        "outlier_models = {\n",
        "    'IsolationForest': (IsolationForest(random_state=0), {\n",
        "        'n_estimators': [200],\n",
        "        'contamination': [0.1]\n",
        "    }),\n",
        "    'OneClassSVM': (OneClassSVM(), {\n",
        "        'nu': [0.05],\n",
        "        'kernel': ['rbf'],\n",
        "        'gamma': ['scale']\n",
        "    }),\n",
        "    'Autoencoder': (AutoencoderModel(input_dim=X.shape[1]), {  # Adjust input_dim based on your dataset\n",
        "        'epochs': [50],\n",
        "        'batch_size': [32]\n",
        "    })\n",
        "}\n",
        "\n",
        "\n",
        "# To evaluate clustering models\n",
        "best_clustering_results = {}\n",
        "for comb in combinations:\n",
        "  print(comb)\n",
        "  # if comb != combinations[-1]:\n",
        "  #     continue\n",
        "  if not comb:\n",
        "      continue\n",
        "  selected_features = set()\n",
        "  for group in comb:\n",
        "      selected_features.update(selected_dict[group])\n",
        "\n",
        "  selected_features = list(selected_features)\n",
        "  X = preprocessing_pipeline.fit_transform(df2[selected_features].head(10000))\n",
        "  # print(selected_features)\n",
        "  for model_name, (model, param_distributions) in clustering_models.items():\n",
        "      print(f\"Running TuneSearchCV for {model_name}\")\n",
        "      search = TuneSearchCV(\n",
        "          model,\n",
        "          param_distributions=param_distributions,\n",
        "          n_trials=10,\n",
        "          scoring=custom_clustering_score,\n",
        "\n",
        "          search_optimization='hyperopt',\n",
        "          n_jobs=-1,\n",
        "          time_budget_s=300,\n",
        "          cv=5,\n",
        "          refit=True,\n",
        "          verbose=1,\n",
        "          error_score=\"raise\",\n",
        "          random_state=0\n",
        "      )\n",
        "      start_time = time.time()\n",
        "      search.fit(X)\n",
        "      end_time = time.time()\n",
        "      elapsed_time = end_time - start_time\n",
        "      # model_filename = f\"{model_dir}/{model_name}_{str(comb)}.joblib\"\n",
        "      # joblib.dump(search.best_estimator_, model_filename)\n",
        "      if hasattr(search, 'best_params_'):\n",
        "        best_clustering_results[model_name] = {\n",
        "            'best_params': search.best_params_,\n",
        "            'best_score': search.best_score_,\n",
        "            'best_cv': search.cv_results_,\n",
        "            'estimator': search.best_estimator_,\n",
        "            'time':elapsed_time\n",
        "        }\n",
        "\n",
        "  # To evaluate outlier models\n",
        "  best_outlier_results = {}\n",
        "  for model_name, (model, param_distributions) in outlier_models.items():\n",
        "      print(f\"Running TuneSearchCV for {model_name}\")\n",
        "      search = TuneSearchCV(\n",
        "          model,\n",
        "          param_distributions=param_distributions,\n",
        "          scoring=custom_outlier_score,\n",
        "          cv=5,\n",
        "\n",
        "          search_optimization='hyperopt',\n",
        "          n_jobs=-1,\n",
        "          time_budget_s=300,\n",
        "          refit=True,\n",
        "          verbose=1,\n",
        "          error_score=\"raise\",\n",
        "          random_state=0\n",
        "      )\n",
        "      start_time = time.time()\n",
        "      search.fit(X)\n",
        "      end_time = time.time()\n",
        "      elapsed_time = end_time - start_time\n",
        "      # model_filename = f\"{model_dir}/{model_name}_{str(comb)}.joblib\"\n",
        "      # joblib.dump(search.best_estimator_, model_filename)\n",
        "      if hasattr(search, 'best_params_'):\n",
        "\n",
        "        best_outlier_results[model_name] = {\n",
        "            'best_params': search.best_params_,\n",
        "            'best_score': search.best_score_,\n",
        "                    'best_cv': search.cv_results_,\n",
        "            'estimator': search.best_estimator_,\n",
        "            'time': elapsed_time\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "  # Find the best clustering model\n",
        "  best_clustering_model_name = None\n",
        "  best_clustering_score = -float('inf')\n",
        "  for model_name, result in best_clustering_results.items():\n",
        "      print(model_name, result['time'])\n",
        "      if result['best_score'] > best_clustering_score:\n",
        "          best_clustering_score = result['best_score']\n",
        "          best_clustering_model_name = model_name\n",
        "          model_filename = f\"{model_dir}/{model_name}_{str(comb)}.joblib\"\n",
        "          joblib.dump(result['estimator'], model_filename)\n",
        "  if best_clustering_model_name:\n",
        "    # Print the best clustering model results\n",
        "    print(\"Best Clustering Model:\")\n",
        "    print(f\"Model: {best_clustering_model_name}\")\n",
        "    print(\"Best Params:\", best_clustering_results[best_clustering_model_name]['best_params'])\n",
        "    print(\"Best Score:\", best_clustering_results[best_clustering_model_name]['best_score'])\n",
        "    print(\"Best cv:\", best_clustering_results[best_clustering_model_name]['best_cv'])\n",
        "\n",
        "\n",
        "  # Find the best outlier model\n",
        "  best_outlier_model_name = None\n",
        "  best_outlier_score = -float('inf')\n",
        "  for model_name, result in best_outlier_results.items():\n",
        "      print(model_name, result['time'])\n",
        "\n",
        "      if result['best_score'] > best_outlier_score:\n",
        "          best_outlier_score = result['best_score']\n",
        "          best_outlier_model_name = model_name\n",
        "          model_filename = f\"{model_dir}/{model_name}_{str(comb)}.joblib\"\n",
        "          joblib.dump(result['estimator'], model_filename)\n",
        "\n",
        "  # Print the best outlier model results\n",
        "  print(\"Best Outlier Model:\")\n",
        "  print(f\"Model: {best_outlier_model_name}\")\n",
        "  print(\"Best Params:\", best_outlier_results[best_outlier_model_name]['best_params'])\n",
        "  print(\"Best Score:\", best_outlier_results[best_outlier_model_name]['best_score'])\n",
        "  print(\"Best cv:\", best_outlier_results[best_outlier_model_name]['best_cv'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDr8CteYy3Rm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, silhouette_score, davies_bouldin_score\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from tune_sklearn import TuneSearchCV\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "\n",
        "# models = {\n",
        "#     'KMeans': (KMeans(random_state=random_state), {\n",
        "#         'n_clusters': [2, 3, 4, 5],\n",
        "#         'init': ['k-means++', 'random'],\n",
        "#         'n_init': [10, 20, 30]\n",
        "#     }),\n",
        "#     'DBSCAN': (DBSCAN(), {\n",
        "#         'eps': [0.3, 0.5, 0.7],\n",
        "#         'min_samples': [5, 10, 20]\n",
        "#     }),\n",
        "#     'IsolationForest': (IsolationForest(random_state=random_state), {\n",
        "#         'n_estimators': [100, 200],\n",
        "#         'contamination': [0.05, 0.1]\n",
        "#     }),\n",
        "#     # 'LocalOutlierFactor': (LocalOutlierFactor(), {\n",
        "#     #     'n_neighbors': [10, 20, 30],\n",
        "#     #     'contamination': [0.05, 0.1]\n",
        "#     # }),\n",
        "#     'OneClassSVM': (OneClassSVM(), {\n",
        "#         'nu': [0.05, 0.1, 0.2],\n",
        "#         'kernel': ['rbf', 'poly'],\n",
        "#         'gamma': ['scale', 'auto']\n",
        "#     })\n",
        "# }\n",
        "# Generate some sample data\n",
        "# data, true_labels = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)\n",
        "def custom_clustering_score(estimator, X, y=None):\n",
        "    labels = estimator.fit_predict(X)\n",
        "    scores = {}\n",
        "    scores['homogeneity'] = homogeneity_score(y, labels)\n",
        "    scores['completeness'] = completeness_score(y, labels)\n",
        "    scores['v_measure'] = v_measure_score(y, labels)\n",
        "    scores['silhouette'] = silhouette_score(X, labels)\n",
        "    scores['davies_bouldin'] = davies_bouldin_score(X, labels)\n",
        "\n",
        "    # Combine the scores (adjust weights as necessary)\n",
        "    combined_score = (scores['homogeneity'] + scores['completeness'] + scores['v_measure'] + scores['silhouette'] - scores['davies_bouldin']) / 5\n",
        "    return combined_score\n",
        "# Define the parameter distribution for KMeans\n",
        "param_distributions = [\n",
        "    # 'model__n_estimators': [100, 200],\n",
        "    #  'model__min_samples_split': [2, 5, 10],\n",
        "    {\n",
        "        # 'reducer': [PCA(), TruncatedSVD()],\n",
        "        'reducer': [SparsePCA()],\n",
        "\n",
        "    'preprocessor__num__scaler': [StandardScaler(), MinMaxScaler()],\n",
        "    'preprocessor__cat__onehot':[OneHotEncoder(handle_unknown=\"ignore\"), TargetEncoder()],\n",
        "          'feature_selection': [ConditionalVarianceThreshold(threshold=0.01)],\n",
        "        # 'model': [AdaBoostClassifier(), RandomForestClassifier()]\n",
        "    'model': [KMeans(),DBSCAN(),IsolationForest(), OneClassSVM()]\n",
        "        }\n",
        "\n",
        "]\n",
        "# Create the TuneSearchCV object\n",
        "search = TuneSearchCV(\n",
        "    pipeline,\n",
        "    param_distributions=param_distributions,\n",
        "    n_trials=10,\n",
        "    scoring=custom_clustering_score,\n",
        "    cv=5,\n",
        "    refit=True,\n",
        "    verbose=1,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "# # Perform the search\n",
        "# search.fit(data, true_labels)\n",
        " # selected= 'ofac_compliant''\n",
        "X = df2.head(100)\n",
        "# Perform the search\n",
        "search.fit(X)\n",
        "\n",
        "# Get the best parameters\n",
        "print(\"Best Params:\", search.best_params_)\n",
        "print(\"Best Score:\", search.best_score_)\n",
        "# # Get the best parameters\n",
        "# print(\"Best Params:\", search.best_params_)\n",
        "# print(\"Best Score:\", search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTTITz3iT8yV"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('best_results_overal2.pkl', 'wb') as file:\n",
        "    pickle.dump(best_results, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti3f70iKnnEM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Flattening best_results to store in a CSV file\n",
        "flattened_results = []\n",
        "for name_comb, result in best_results.items():\n",
        "    result_flat = {\n",
        "        'search_strategy_combination': name_comb,\n",
        "        'best_estimator': str(result['best_estimator']),\n",
        "        'best_score': result['best_score'],\n",
        "        'best_params': result['best_params'],\n",
        "        'best_index': result['best_index'],\n",
        "        'time_spent': result['time_spent'],\n",
        "    }\n",
        "    # Flattening cv_results\n",
        "    cv_results_df = pd.DataFrame(result['cv_results'])\n",
        "    for col in cv_results_df.columns:\n",
        "        result_flat[f'cv_results_{col}'] = list(cv_results_df[col])\n",
        "    flattened_results.append(result_flat)\n",
        "\n",
        "# Creating DataFrame from flattened results\n",
        "results_df = pd.DataFrame(flattened_results)\n",
        "\n",
        "# Saving the DataFrame to a CSV file\n",
        "filename = \"/content/drive/MyDrive/datasets/randomcv.csv\"\n",
        "results_df.to_csv('filename', index=False)\n",
        "\n",
        "# Display the best results\n",
        "for name, result in best_results.items():\n",
        "    print(f\"Best model with {name}: \", result['best_estimator'])\n",
        "    print(f\"Best score with {name}: \", result['best_score'])\n",
        "    print(f\"Best parameters with {name}: \", result['best_params'])\n",
        "    print(f\"Best index with {name}: \", result['best_index'])\n",
        "    print(f\"CV results with {name}: \", pd.DataFrame(result['cv_results']).head())\n",
        "    print(f\"Time spent with {name}: {result['time_spent']} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNFIIiPuPPFB"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "import csv\n",
        "from collections import OrderedDict\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "data = best_results\n",
        "# Function to flatten the dictionary\n",
        "def flatten_dict(d, parent_key='', sep='__'):\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "        if isinstance(v, dict) or isinstance(v, OrderedDict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "# Flatten the dictionary\n",
        "flat_data = {k: flatten_dict(v) for k, v in data.items()}\n",
        "\n",
        "# Prepare the CSV file\n",
        "filename = \"/content/drive/MyDrive/datasets/test4.csv\"\n",
        "\n",
        "# Get the headers from the first entry\n",
        "headers = set()\n",
        "for entry in flat_data.values():\n",
        "    headers.update(entry.keys())\n",
        "headers = sorted(headers)\n",
        "\n",
        "# Write to the CSV file\n",
        "with open(filename, 'w', newline='') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=['method'] + headers)\n",
        "    writer.writeheader()\n",
        "    for method, params in flat_data.items():\n",
        "        row = {'method': method}\n",
        "        row.update(params)\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"Data has been written to {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2aqeLWckJmQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# X = df2['ofac_compliant'].head(100)\n",
        "start_index = 100\n",
        "end_index = 100000\n",
        "\n",
        "# Ensure we don't go out of bounds\n",
        "end_index = min(end_index, len(df2))\n",
        "\n",
        "# Select the rows\n",
        "selected_rows = df2.iloc[start_index:end_index]\n",
        "\n",
        "# Drop the 'ofac_compliant' column and separate the target column\n",
        "X = selected_rows.drop(columns=['ofac_compliant'])\n",
        "y = selected_rows['ofac_compliant']\n",
        "\n",
        "# y = df2['ofac_compliant'].head(100)\n",
        "# Function to load a model given its file path\n",
        "def load_model(model_filepath):\n",
        "    return joblib.load(model_filepath)\n",
        "\n",
        "# Dictionary to store loaded models\n",
        "loaded_models = {}\n",
        "\n",
        "\n",
        "# Prepare your new dataset for evaluation\n",
        "\n",
        "# Evaluate the performance of each loaded model on the new data\n",
        "evaluation_results = []\n",
        "model = load_model('/content/drive/MyDrive/datasets/saved_models/HalvingRandomSearchCV_1057203736142586241.joblib')\n",
        "# for model_name, model in loaded_models.items():\n",
        "# Make predictions on the new data\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "precision = precision_score(y, y_pred, average='weighted')\n",
        "recall = recall_score(y, y_pred, average='weighted')\n",
        "f1 = f1_score(y, y_pred, average='weighted')\n",
        "\n",
        "# Store the evaluation results\n",
        "evaluation_results.append({\n",
        "    # 'model_name': model_name,\n",
        "    'accuracy': accuracy,\n",
        "    'precision': precision,\n",
        "    'recall': recall,\n",
        "    'f1_score': f1\n",
        "})\n",
        "\n",
        "# Create a DataFrame from the evaluation results\n",
        "evaluation_df = pd.DataFrame(evaluation_results)\n",
        "\n",
        "# Save the evaluation results to a CSV file\n",
        "# evaluation_df.to_csv('model_evaluation_results.csv', index=False)\n",
        "\n",
        "# Display the evaluation results\n",
        "display(evaluation_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WluD3nNWD4S2"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "import csv\n",
        "from collections import OrderedDict\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "data = best_results\n",
        "# Function to flatten the dictionary\n",
        "def flatten_dict(d, parent_key='', sep='__'):\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "        if isinstance(v, dict) or isinstance(v, OrderedDict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "# Flatten the dictionary\n",
        "flat_data = {k: flatten_dict(v) for k, v in data.items()}\n",
        "\n",
        "# Prepare the CSV file\n",
        "filename = \"/content/drive/MyDrive/datasets/test2.csv\"\n",
        "\n",
        "# Get the headers from the first entry\n",
        "headers = set()\n",
        "for entry in flat_data.values():\n",
        "    headers.update(entry.keys())\n",
        "headers = sorted(headers)\n",
        "\n",
        "# Write to the CSV file\n",
        "with open(filename, 'w', newline='') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=['method'] + headers)\n",
        "    writer.writeheader()\n",
        "    for method, params in flat_data.items():\n",
        "        row = {'method': method}\n",
        "        row.update(params)\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"Data has been written to {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0z6LoUtAlMl"
      },
      "outputs": [],
      "source": [
        "param_grid = pipeline.get_params()\n",
        "\n",
        "# Display the parameters\n",
        "for key in param_grid:\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B6vUHLbOCjX"
      },
      "outputs": [],
      "source": [
        "\"\"\"Example using an sklearn Pipeline with early stopping.\n",
        "\n",
        "Example taken and modified from\n",
        "https://scikit-learn.org/stable/auto_examples/compose/\n",
        "plot_compare_reduction.html\n",
        "\"\"\"\n",
        "\n",
        "from tune_sklearn import TuneSearchCV\n",
        "from tune_sklearn import TuneGridSearchCV\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "# partial_fit\n",
        "\n",
        "# pipe = Pipeline([(\"reduce_dim\", PCA()), (\"classify\", SGDClassifier())])\n",
        "param_grid = {\n",
        "    'model__n_estimators': [100, 200],\n",
        "    # 'model__max_depth': [None, 10, 20, 30],\n",
        "    'model__min_samples_split': [2, 5, 10],\n",
        "    # 'model__min_samples_leaf': [1, 2, 4],\n",
        "    # 'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
        "    # 'preprocessor__cat__imputer__strategy': ['most_frequent', 'constant'],\n",
        "    # 'preprocessor__cat__onehot__handle_unknown': ['ignore', 'error']\n",
        "}\n",
        "\n",
        "random = TuneSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    search_optimization=\"random\",\n",
        "    # early_stopping=True,\n",
        "    max_iters=10,\n",
        "    # model__min_samples_split,\n",
        "    pipeline_auto_early_stop=True)\n",
        "\n",
        "\n",
        "random.fit(X, y)\n",
        "print(random.best_params_)\n",
        "print(random.best_estimator_.named_steps['model'])\n",
        "print(random.best_score_)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i371VyU9mNvj"
      },
      "outputs": [],
      "source": [
        "from tune_sklearn import TuneSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ray.tune.schedulers import MedianStoppingRule\n",
        "import numpy as np\n",
        "\n",
        "digits = datasets.load_digits()\n",
        "x = digits.data\n",
        "y = digits.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)\n",
        "\n",
        "clf = SGDClassifier()\n",
        "parameter_grid = {\"alpha\": (1e-4, 1), \"epsilon\": (0.01, 0.1)}\n",
        "\n",
        "scheduler = MedianStoppingRule(grace_period=10.0)\n",
        "\n",
        "tune_search = TuneSearchCV(\n",
        "    clf,\n",
        "    parameter_grid,\n",
        "    search_optimization=\"bayesian\",\n",
        "    n_trials=3,\n",
        "    early_stopping=scheduler,\n",
        "    max_iters=10)\n",
        "tune_search.fit(x_train, y_train)\n",
        "\n",
        "pred = tune_search.predict(x_test)\n",
        "accuracy = np.count_nonzero(np.array(pred) == np.array(y_test)) / len(pred)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GytMy5Ewnxs0"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "from tune_sklearn import TuneGridSearchCV\n",
        "\n",
        "# Other imports\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Set training and validation sets\n",
        "X, y = make_classification(n_samples=11000, n_features=1000, n_informative=50, n_redundant=0, n_classes=10, class_sep=2.5)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1000)\n",
        "\n",
        "# Example parameters to tune from SGDClassifier\n",
        "parameters = {\n",
        "    'alpha': [1e-4, 1e-1, 1],\n",
        "    'epsilon':[0.01, 0.1]\n",
        "}\n",
        "\n",
        "tune_search = TuneGridSearchCV(\n",
        "    SGDClassifier(),\n",
        "    parameters,\n",
        "    early_stopping=\"MedianStoppingRule\",\n",
        "    max_iters=10\n",
        ")\n",
        "\n",
        "import time # Just to compare fit times\n",
        "start = time.time()\n",
        "tune_search.fit(X_train, y_train)\n",
        "end = time.time()\n",
        "print(\"Tune Fit Time:\", end - start)\n",
        "pred = tune_search.predict(X_test)\n",
        "accuracy = np.count_nonzero(np.array(pred) == np.array(y_test)) / len(pred)\n",
        "print(\"Tune Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPv7g25LouvH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# n_jobs=-1 enables use of all cores like Tune does\n",
        "sklearn_search = GridSearchCV(\n",
        "    SGDClassifier(),\n",
        "    parameters,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "sklearn_search.fit(X_train, y_train)\n",
        "end = time.time()\n",
        "print(\"Sklearn Fit Time:\", end - start)\n",
        "pred = sklearn_search.predict(X_test)\n",
        "accuracy = np.count_nonzero(np.array(pred) == np.array(y_test)) / len(pred)\n",
        "print(\"Sklearn Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InHo7fdxMnhZ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, LeaveOneOut, GroupKFold\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tune_sklearn import TuneGridSearchCV, TuneSearchCV\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from skopt import BayesSearchCV\n",
        "from category_encoders import TargetEncoder\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import ray\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "# def generate_synthetic_data():\n",
        "#     X_num, y = make_classification(n_samples=1000, n_features=15, n_informative=10, n_redundant=5, random_state=42)\n",
        "#     X_cat = np.random.choice(['A', 'B', 'C'], size=(1000, 5))\n",
        "#     X_bool = np.random.choice([0, 1], size=(1000, 2))\n",
        "#     X = np.hstack((X_num, X_cat, X_bool))\n",
        "#     feature_names = [f'num_{i}' for i in range(15)] + [f'cat_{i}' for i in range(5)] + [f'bool_{i}' for i in range(2)]\n",
        "#     df = pd.DataFrame(X, columns=feature_names)\n",
        "#     df['target'] = y\n",
        "#     return df\n",
        "\n",
        "# data = generate_synthetic_data()\n",
        "\n",
        "# data.to_csv('synthetic_dataset.csv', index=False)\n",
        "# dtype = {f'cat_{i}': 'object' for i in range(5)}\n",
        "# data = dd.read_csv('synthetic_dataset.csv', dtype=dtype)\n",
        "# data = data.compute()\n",
        "# X = data.iloc[:, :-1]\n",
        "# y = data.iloc[:, -1]\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path = '/content/drive/MyDrive/datasets/new2.csv'\n",
        "\n",
        "# Define the dtypes for problematic columns\n",
        "dtype = {\n",
        "    'builder': 'object',\n",
        "    'relay': 'object'\n",
        "    # Add other columns if necessary\n",
        "}\n",
        "\n",
        "# Load the dataset\n",
        "data = dd.read_csv(file_path, dtype=dtype)\n",
        "data = data.compute().head(2000)\n",
        "drop_columns = ['relay', 'mevboost_value', 'builder', 'day', 'validator']\n",
        "data = data.drop(columns=drop_columns, errors='ignore')\n",
        "def check_nan_values(df):\n",
        "    nan_summary = df.isna().sum()\n",
        "    print(\"NaN values summary:\")\n",
        "    print(nan_summary[nan_summary > 0])\n",
        "check_nan_values(data)\n",
        "# Define your features and target\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "numerical_transformers = {\n",
        "    'standard': StandardScaler(),\n",
        "    'minmax': MinMaxScaler(),\n",
        "    'power': PowerTransformer()\n",
        "}\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "preprocessor_onehot = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', 'passthrough', numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "preprocessor_target = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', 'passthrough', numerical_features),\n",
        "        ('cat', TargetEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "variance_selector = VarianceThreshold(threshold=0.1)\n",
        "kbest_selector = SelectKBest(score_func=f_classif, k=10)\n",
        "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=10)\n",
        "\n",
        "models = [\n",
        "    MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, early_stopping=True, warm_start=True),\n",
        "    LogisticRegression(),\n",
        "    KNeighborsClassifier(n_neighbors=5),\n",
        "    RandomForestClassifier(),\n",
        "    AdaBoostClassifier(),\n",
        "    SVC(),\n",
        "    GaussianNB()\n",
        "]\n",
        "\n",
        "pipelines = {\n",
        "    'onehot': Pipeline(steps=[('preprocessor', preprocessor_onehot), ('selector', 'passthrough'), ('model', 'passthrough')]),\n",
        "    'target': Pipeline(steps=[('preprocessor', preprocessor_target), ('selector', 'passthrough'), ('model', 'passthrough')])\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'onehot': {\n",
        "        'preprocessor__num': list(numerical_transformers.values()),\n",
        "        'selector': [variance_selector, kbest_selector, rfe_selector],\n",
        "        'model': models\n",
        "    },\n",
        "    'target': {\n",
        "        'preprocessor__num': list(numerical_transformers.values()),\n",
        "        'selector': [variance_selector, kbest_selector, rfe_selector],\n",
        "        'model': models\n",
        "    }\n",
        "}\n",
        "\n",
        "def run_search_with_timeout(search, X_train, y_train, timeout):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        search.fit(X_train, y_train)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        if elapsed_time > timeout:\n",
        "            raise TimeoutError(f\"{search.__class__.__name__} timed out\")\n",
        "        return search.best_params_, search.best_score_\n",
        "    except Exception as e:\n",
        "        print(f\"{search.__class__.__name__} encountered an error: {e}\")\n",
        "        return None, None\n",
        "\n",
        "timeout = 240\n",
        "\n",
        "cv_methods = {\n",
        "    'KFold': StratifiedKFold(n_splits=3),\n",
        "    'LeaveOneOut': LeaveOneOut(),\n",
        "    'GroupKFold': GroupKFold(n_splits=3)\n",
        "}\n",
        "\n",
        "search_strategies = [\n",
        "    ('GridSearchCV', GridSearchCV, {'n_jobs': -1, 'scoring': 'accuracy', 'error_score': 'raise'}),\n",
        "    ('RandomizedSearchCV', RandomizedSearchCV, {'n_iter': 5, 'n_jobs': -1, 'scoring': 'accuracy', 'error_score': 'raise'}),\n",
        "    ('HalvingGridSearchCV', HalvingGridSearchCV, {'factor': 2, 'n_jobs': -1, 'scoring': 'accuracy', 'error_score': 'raise'}),\n",
        "    ('HalvingRandomSearchCV', HalvingRandomSearchCV, {'factor': 2, 'n_jobs': -1, 'scoring': 'accuracy', 'error_score': 'raise'}),\n",
        "    ('BayesSearchCV', BayesSearchCV, {'n_iter': 10, 'n_jobs': -1, 'scoring': 'accuracy', 'error_score': 'raise'}),\n",
        "    ('TuneGridSearchCV', TuneGridSearchCV, {'max_iters': 10, 'n_jobs': -1, 'scoring': 'accuracy', 'error_score': 'raise'}),\n",
        "    ('TuneSearchCV', TuneSearchCV, {'n_jobs': -1, 'scoring': 'accuracy', 'early_stopping': False, 'error_score': 'raise'})\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for preprocessor_name, pipeline in pipelines.items():\n",
        "    param_grid = param_grids[preprocessor_name]\n",
        "    for name, search_class, search_params in search_strategies:\n",
        "        search = search_class(pipeline, param_grid, cv=cv_methods['KFold'], **search_params)\n",
        "        start_time = time.time()\n",
        "        best_params, best_score = run_search_with_timeout(search, X_train, y_train, timeout)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        results.append({\n",
        "            'Preprocessor': preprocessor_name,\n",
        "            'Method': name,\n",
        "            'Best Params': best_params,\n",
        "            'Best Score': best_score,\n",
        "            'Time Taken (s)': elapsed_time\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "\n",
        "validation_results = []\n",
        "for result in results:\n",
        "    if result['Best Params'] is not None:\n",
        "        best_pipeline = pipelines[result['Preprocessor']].set_params(**result['Best Params'])\n",
        "        best_pipeline.fit(X_train, y_train)\n",
        "        val_predictions = best_pipeline.predict(X_val)\n",
        "        val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "        validation_results.append({\n",
        "            'Preprocessor': result['Preprocessor'],\n",
        "            'Method': result['Method'],\n",
        "            'Validation Accuracy': val_accuracy\n",
        "        })\n",
        "\n",
        "validation_results_df = pd.DataFrame(validation_results)\n",
        "print(validation_results_df)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X.select_dtypes(include=['int64', 'float64']))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
        "plt.title('PCA of Dataset')\n",
        "plt.show()\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "X_tsne = tsne.fit_transform(X.select_dtypes(include=['int64', 'float64']))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
        "plt.title('t-SNE of Dataset')\n",
        "plt.show()\n",
        "\n",
        "best_result = results_df.loc[results_df['Best Score'].idxmax()]\n",
        "best_pipeline = pipelines[best_result['Preprocessor']]\n",
        "best_params = best_result['Best Params']\n",
        "best_pipeline.set_params(**best_params)\n",
        "best_pipeline.fit(X_train, y_train)\n",
        "joblib.dump(best_pipeline, 'best_pipeline.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtZe_mqvTnIm"
      },
      "outputs": [],
      "source": [
        "best_result = results_df.loc[results_df['Best Score'].idxmax()]\n",
        "best_pipeline = pipelines[best_result['Preprocessor']]\n",
        "best_params = best_result['Best Params']\n",
        "\n",
        "# joblib.dump(best_pipeline, 'best_pipeline.pkl')\n",
        "print(best_result)\n",
        "print(best_params)\n",
        "print(best_pipeline)\n",
        "best_pipeline.set_params(**best_params)\n",
        "best_pipeline.fit(X_train, y_train)\n",
        "# Predict using the best_pipeline\n",
        "new_predictions = best_pipeline.predict(X)\n",
        "\n",
        "# Calculate accuracy on the new data\n",
        "new_data_accuracy = accuracy_score(y, new_predictions)\n",
        "\n",
        "print(\"Best Model Details:\")\n",
        "print(best_result)\n",
        "print(\"Best Parameters:\")\n",
        "print(best_params)\n",
        "print(\"Pipeline Used:\")\n",
        "print(best_pipeline)\n",
        "print(\"Accuracy on New Data: {:.2f}%\".format(new_data_accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POuAiv5RS4gU"
      },
      "outputs": [],
      "source": [
        "display(validation_results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NW_8dlMTHPY"
      },
      "outputs": [],
      "source": [
        "display(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCHu4-DETe5Q"
      },
      "outputs": [],
      "source": [
        "# @title Best Score vs Time Taken (s)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "results_df.plot(kind='scatter', x='Best Score', y='Time Taken (s)', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lO84t9hTX3u"
      },
      "outputs": [],
      "source": [
        "# @title Time Taken (s) vs Best Score\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "def _plot_series(series, series_name, series_index=0):\n",
        "  palette = list(sns.palettes.mpl_palette('Dark2'))\n",
        "  xs = series['Time Taken (s)']\n",
        "  ys = series['Best Score']\n",
        "\n",
        "  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')\n",
        "df_sorted = results_df.sort_values('Time Taken (s)', ascending=True)\n",
        "for i, (series_name, series) in enumerate(df_sorted.groupby('Preprocessor')):\n",
        "  _plot_series(series, series_name, i)\n",
        "  fig.legend(title='Preprocessor', bbox_to_anchor=(1, 1), loc='upper left')\n",
        "sns.despine(fig=fig, ax=ax)\n",
        "plt.xlabel('Time Taken (s)')\n",
        "_ = plt.ylabel('Best Score')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}